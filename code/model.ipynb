{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train cv\n",
    "## optimize all\n",
    "## submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_kg_hide-input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "# import timeb\n",
    "from collections import Counter\n",
    "import datetime\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit, RepeatedStratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from bayes_opt import BayesianOptimization\n",
    "import eli5\n",
    "import shap\n",
    "from IPython.display import HTML\n",
    "import json\n",
    "import altair as alt\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from typing import List\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import gc\n",
    "from numba import jit\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "from typing import Any\n",
    "from itertools import product\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "random_seed = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('../data/preprocess/train895.csv')\n",
    "# test = pd.read_csv('../data/preprocess/test895.csv')\n",
    "# train = pd.read_csv('../data/preprocess/train_1225.csv')\n",
    "# test = pd.read_csv('../data/preprocess/test_1225.csv')\n",
    "train = pd.read_csv('../data/preprocess/train_1225_2.csv')\n",
    "test = pd.read_csv('../data/preprocess/test_1225_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "def lgb_kappa_eval(y_true, y_pred):\n",
    "    loss = cohen_kappa_score(y_true, y_pred,weights='quadratic')\n",
    "    return \"qwk\", loss, True\n",
    "\n",
    "def lgb_kappa_eval2(a1, a2):\n",
    "    max_rat = 3\n",
    "    a1 = np.asarray(a1, dtype=int)\n",
    "    a2 = np.asarray(a2, dtype=int)\n",
    "\n",
    "    hist1 = np.zeros((max_rat + 1, ))\n",
    "    hist2 = np.zeros((max_rat + 1, ))\n",
    "\n",
    "    o = 0\n",
    "    for k in range(a1.shape[0]):\n",
    "        i, j = a1[k], a2[k]\n",
    "        hist1[i] += 1\n",
    "        hist2[j] += 1\n",
    "        o +=  (i - j) * (i - j)\n",
    "\n",
    "    e = 0\n",
    "    for i in range(max_rat + 1):\n",
    "        for j in range(max_rat + 1):\n",
    "            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n",
    "\n",
    "    e = e / a1.shape[0]\n",
    "\n",
    "    return \"qwk2\", 1 - o / e, True\n",
    "\n",
    "## https://github.com/catboost/tutorials/blob/master/custom_loss/custom_loss_and_metric_tutorial.ipynb\n",
    "class catboost_qwk(object):\n",
    "    def get_final_error(self, error, weight):\n",
    "        return error / (weight + 1e-38)\n",
    "\n",
    "    def is_max_optimal(self):\n",
    "        return True\n",
    "\n",
    "    def evaluate(self, approxes, target, weight):\n",
    "        # approxes is list of indexed containers\n",
    "        # (containers with only __len__ and __getitem__ defined), one container\n",
    "        # per approx dimension. Each container contains floats.\n",
    "        # weight is one dimensional indexed container.\n",
    "        # target is float.   \n",
    "        # weight parameter can be None.\n",
    "        # Returns pair (error, weights sum)\n",
    "\n",
    "        assert len(approxes) == 1\n",
    "        assert len(target) == len(approxes[0])\n",
    "\n",
    "        approx = approxes[0]\n",
    "\n",
    "        error_sum = 0.0\n",
    "        weight_sum = 1.0\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for i in range(len(approx)):\n",
    "            y_true.append(target[i])\n",
    "            y_pred.append(approx[i])\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_true = np.array(y_true)\n",
    "        \n",
    "        y_pred[y_pred <= 1.12232214] = 0\n",
    "        y_pred[np.where(np.logical_and(y_pred > 1.12232214, y_pred <= 1.73925866))] = 1\n",
    "        y_pred[np.where(np.logical_and(y_pred > 1.73925866, y_pred <= 2.22506454))] = 2\n",
    "        y_pred[y_pred > 2.22506454] = 3\n",
    "        \n",
    "        error_sum = cohen_kappa_score(y_true, y_pred,weights='quadratic')\n",
    "#         error_sum = qwk(y_true, y_pred)\n",
    "        \n",
    "        return error_sum, weight_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(CatBoostRegressor())==CatBoostRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "@jit\n",
    "def qwk(a1, a2):\n",
    "    \"\"\"\n",
    "    Source: https://www.kaggle.com/c/data-science-bowl-2019/discussion/114133#latest-660168\n",
    "\n",
    "    :param a1:\n",
    "    :param a2:\n",
    "    :param max_rat:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    max_rat = 3\n",
    "    a1 = np.asarray(a1, dtype=int)\n",
    "    a2 = np.asarray(a2, dtype=int)\n",
    "\n",
    "    hist1 = np.zeros((max_rat + 1, ))\n",
    "    hist2 = np.zeros((max_rat + 1, ))\n",
    "\n",
    "    o = 0\n",
    "    for k in range(a1.shape[0]):\n",
    "        i, j = a1[k], a2[k]\n",
    "        hist1[i] += 1\n",
    "        hist2[j] += 1\n",
    "        o +=  (i - j) * (i - j)\n",
    "\n",
    "    e = 0\n",
    "    for i in range(max_rat + 1):\n",
    "        for j in range(max_rat + 1):\n",
    "            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n",
    "\n",
    "    e = e / a1.shape[0]\n",
    "\n",
    "    return 1 - o / e\n",
    "\n",
    "def eval_qwk_lgb_regr(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Fast cappa eval function for lgb.\n",
    "    \"\"\"\n",
    "    y_pred[y_pred <= 1.12232214] = 0\n",
    "    y_pred[np.where(np.logical_and(y_pred > 1.12232214, y_pred <= 1.73925866))] = 1\n",
    "    y_pred[np.where(np.logical_and(y_pred > 1.73925866, y_pred <= 2.22506454))] = 2\n",
    "    y_pred[y_pred > 2.22506454] = 3\n",
    "\n",
    "    # y_pred = y_pred.reshape(len(np.unique(y_true)), -1).argmax(axis=0)\n",
    "\n",
    "    return 'cappa', qwk(y_true, y_pred), True\n",
    "\n",
    "\n",
    "class LGBWrapper_regr(object):\n",
    "    \"\"\"\n",
    "    A wrapper for lightgbm model so that we will have a single api for various models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = lgb.LGBMRegressor()\n",
    "\n",
    "    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None, y_holdout=None, params=None):\n",
    "        eval_metric = eval_qwk_lgb_regr\n",
    "        \n",
    "        eval_set = [(X_train, y_train)]\n",
    "        eval_names = ['train']\n",
    "        self.model = self.model.set_params(**params)\n",
    "\n",
    "        if X_valid is not None:\n",
    "            eval_set.append((X_valid, y_valid))\n",
    "            eval_names.append('valid')\n",
    "\n",
    "        if X_holdout is not None:\n",
    "            eval_set.append((X_holdout, y_holdout))\n",
    "            eval_names.append('holdout')\n",
    "\n",
    "        if 'cat_cols' in params.keys():\n",
    "            cat_cols = [col for col in params['cat_cols'] if col in X_train.columns]\n",
    "            if len(cat_cols) > 0:\n",
    "                categorical_columns = params['cat_cols']\n",
    "            else:\n",
    "                categorical_columns = 'auto'\n",
    "        else:\n",
    "            categorical_columns = 'auto'\n",
    "\n",
    "#                        eval_set=eval_set, eval_names=eval_names, eval_metric=lgb_kappa_eval,\n",
    "        self.model.fit(X=X_train, y=y_train,\n",
    "                       eval_set=eval_set, eval_names=eval_names, eval_metric=eval_metric,\n",
    "                       verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'],\n",
    "                       categorical_feature=categorical_columns)\n",
    "\n",
    "        self.best_score_ = self.model.best_score_\n",
    "        self.feature_importances_ = self.model.feature_importances_\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test, num_iteration=self.model.best_iteration_)\n",
    "\n",
    "\n",
    "class CatboostWrapper_regr(object):\n",
    "    \"\"\"\n",
    "    A wrapper for catboost model so that we will have a single api for various models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = CatBoostRegressor()\n",
    "\n",
    "    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None, y_holdout=None, params=None):\n",
    "        eval_set = []\n",
    "        eval_names = []\n",
    "        self.model = self.model.set_params(**params)\n",
    "        self.model = self.model.set_params(eval_metric=catboost_qwk())\n",
    "        \n",
    "        \n",
    "        if X_valid is not None:\n",
    "            eval_set.append((X_valid, y_valid))\n",
    "            eval_names.append('valid')\n",
    "        elif X_holdout is not None:\n",
    "            eval_set.append((X_holdout, y_holdout))\n",
    "            eval_names.append('holdout')\n",
    "        else:\n",
    "            eval_set.append((X_train, y_train))\n",
    "            eval_names.append('train')\n",
    "        \n",
    "        if 'cat_cols' in params.keys():\n",
    "            cat_cols = [col for col in params['cat_cols'] if col in X_train.columns]\n",
    "            if len(cat_cols) > 0:\n",
    "                categorical_columns = params['cat_cols']\n",
    "            else:\n",
    "                categorical_columns = None\n",
    "        else:\n",
    "            categorical_columns = None\n",
    "\n",
    "        self.model.fit(X=X_train, y=y_train,\n",
    "                       eval_set=eval_set,\n",
    "                       verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'],\n",
    "                       cat_features=categorical_columns)\n",
    "\n",
    "        self.best_score_ = self.model.get_best_score()\n",
    "        \n",
    "        test_pool=Pool(X_valid, y_valid,cat_features=categorical_columns)\n",
    "        self.feature_importances_ = self.model.get_feature_importance(test_pool,thread_count=4)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test, ntree_end=self.model.get_best_iteration())\n",
    "\n",
    "\n",
    "\n",
    "class MainTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, convert_cyclical: bool = False, create_interactions: bool = False, n_interactions: int = 20):\n",
    "        \"\"\"\n",
    "        Main transformer for the data. Can be used for processing on the whole data.\n",
    "\n",
    "        :param convert_cyclical: convert cyclical features into continuous\n",
    "        :param create_interactions: create interactions between features\n",
    "        \"\"\"\n",
    "\n",
    "        self.convert_cyclical = convert_cyclical\n",
    "        self.create_interactions = create_interactions\n",
    "        self.feats_for_interaction = None\n",
    "        self.n_interactions = n_interactions\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        if self.create_interactions:\n",
    "            self.feats_for_interaction = [col for col in X.columns if 'sum' in col\n",
    "                                          or 'mean' in col or 'max' in col or 'std' in col\n",
    "                                          or 'attempt' in col]\n",
    "            self.feats_for_interaction1 = np.random.choice(self.feats_for_interaction, self.n_interactions)\n",
    "            self.feats_for_interaction2 = np.random.choice(self.feats_for_interaction, self.n_interactions)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        data = copy.deepcopy(X)\n",
    "        if self.create_interactions:\n",
    "            for col1 in self.feats_for_interaction1:\n",
    "                for col2 in self.feats_for_interaction2:\n",
    "                    data[f'{col1}_int_{col2}'] = data[col1] * data[col2]\n",
    "\n",
    "        if self.convert_cyclical:\n",
    "            data['timestampHour'] = np.sin(2 * np.pi * data['timestampHour'] / 23.0)\n",
    "            data['timestampMonth'] = np.sin(2 * np.pi * data['timestampMonth'] / 23.0)\n",
    "            data['timestampWeek'] = np.sin(2 * np.pi * data['timestampWeek'] / 23.0)\n",
    "            data['timestampMinute'] = np.sin(2 * np.pi * data['timestampMinute'] / 23.0)\n",
    "\n",
    "#         data['installation_session_count'] = data.groupby(['installation_id'])['Clip'].transform('count')\n",
    "#         data['installation_duration_mean'] = data.groupby(['installation_id'])['duration_mean'].transform('mean')\n",
    "#         data['installation_title_nunique'] = data.groupby(['installation_id'])['session_title'].transform('nunique')\n",
    "\n",
    "#         data['sum_event_code_count'] = data[['2000', '3010', '3110', '4070', '4090', '4030', '4035', '4021', '4020', '4010', '2080', '2083', '2040', '2020', '2030', '3021', '3121', '2050', '3020', '3120', '2060', '2070', '4031', '4025', '5000', '5010', '2081', '2025', '4022', '2035', '4040', '4100', '2010', '4110', '4045', '4095', '4220', '2075', '4230', '4235', '4080', '4050']].sum(axis=1)\n",
    "\n",
    "        # data['installation_event_code_count_mean'] = data.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n",
    "\n",
    "        return data\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        data = copy.deepcopy(X)\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "\n",
    "\n",
    "class FeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, main_cat_features: list = None, num_cols: list = None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param main_cat_features:\n",
    "        :param num_cols:\n",
    "        \"\"\"\n",
    "        self.main_cat_features = main_cat_features\n",
    "        self.num_cols = num_cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "#         self.num_cols = [col for col in X.columns if 'sum' in col or 'mean' in col or 'max' in col or 'std' in col\n",
    "#                          or 'attempt' in col]\n",
    "        \n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        data = copy.deepcopy(X)\n",
    "#         for col in self.num_cols:\n",
    "#             data[f'{col}_to_mean'] = data[col] / data.groupby('installation_id')[col].transform('mean')\n",
    "#             data[f'{col}_to_std'] = data[col] / data.groupby('installation_id')[col].transform('std')\n",
    "\n",
    "        return data\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        data = copy.deepcopy(X)\n",
    "        self.fit(data)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class RegressorModel(object):\n",
    "    \"\"\"\n",
    "    A wrapper class for regression models.\n",
    "    It can be used for training and prediction.\n",
    "    Can plot feature importance and training progress (if relevant for model).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, columns: list=None, model_wrapper=None, random_state=33):\n",
    "        \"\"\"\n",
    "        :param columns: columns to train\n",
    "        :param model_wrapper:\n",
    "        \"\"\"\n",
    "        self.columns = columns\n",
    "        self.model_wrapper = model_wrapper\n",
    "        self.random_seed = random_state\n",
    "        \n",
    "        self.result_dict = {}\n",
    "        self.train_one_fold = False\n",
    "        self.preprocesser = None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y,\n",
    "            X_holdout: pd.DataFrame = None, y_holdout=None,\n",
    "            folds=None,\n",
    "            params: dict = None,\n",
    "            eval_metric='rmse',\n",
    "            cols_to_drop: list = None,\n",
    "            preprocesser=None,\n",
    "            transformers: dict = None,\n",
    "            adversarial: bool = False,\n",
    "            plot: bool = True):\n",
    "        \"\"\"\n",
    "        Training the model.\n",
    "\n",
    "        :param X: training data\n",
    "        :param y: training target\n",
    "        :param X_holdout: holdout data\n",
    "        :param y_holdout: holdout target\n",
    "        :param folds: folds to split the data. If not defined, then model will be trained on the whole X\n",
    "        :param params: training parameters\n",
    "        :param eval_metric: metric for validataion\n",
    "        :param cols_to_drop: list of columns to drop (for example ID)\n",
    "        :param preprocesser: preprocesser class\n",
    "        :param transformers: transformer to use on folds\n",
    "        :param adversarial\n",
    "        :return:\n",
    "        \"\"\"\n",
    "# transformers,n_target,oof,\n",
    "\n",
    "        if folds is None:\n",
    "            folds = KFold(n_splits=3, random_state=self.random_seed)\n",
    "            self.train_one_fold = True\n",
    "\n",
    "        self.columns = X.columns if self.columns is None else self.columns\n",
    "        self.feature_importances = pd.DataFrame(columns=['feature', 'importance'])\n",
    "        self.trained_transformers = {k: [] for k in transformers}\n",
    "        self.transformers = transformers\n",
    "        self.models = []\n",
    "        self.folds_dict = {}\n",
    "        self.eval_metric = eval_metric\n",
    "        n_target = 1\n",
    "        self.oof = np.zeros((len(X), n_target))\n",
    "        self.n_target = n_target\n",
    "\n",
    "        X = X[self.columns]\n",
    "        if X_holdout is not None:\n",
    "            X_holdout = X_holdout[self.columns]\n",
    "\n",
    "        if preprocesser is not None:\n",
    "            self.preprocesser = preprocesser\n",
    "            self.preprocesser.fit(X, y)\n",
    "            X = self.preprocesser.transform(X, y)\n",
    "            self.columns = X.columns.tolist()\n",
    "            if X_holdout is not None:\n",
    "                X_holdout = self.preprocesser.transform(X_holdout)\n",
    "\n",
    "        for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y, X['installation_id'])):\n",
    "\n",
    "            if X_holdout is not None:\n",
    "                X_hold = X_holdout.copy()\n",
    "            else:\n",
    "                X_hold = None\n",
    "            self.folds_dict[fold_n] = {}\n",
    "            if params['verbose']:\n",
    "                print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
    "#             self.folds_dict[fold_n] = {}\n",
    "\n",
    "            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "            if self.train_one_fold:\n",
    "                X_train = X[self.original_columns]\n",
    "                y_train = y\n",
    "                X_valid = None\n",
    "                y_valid = None\n",
    "\n",
    "            datasets = {'X_train': X_train, 'X_valid': X_valid, 'X_holdout': X_hold, 'y_train': y_train}\n",
    "            X_train, X_valid, X_hold = self.transform_(datasets, cols_to_drop)\n",
    "\n",
    "            self.folds_dict[fold_n]['columns'] = X_train.columns.tolist()\n",
    "\n",
    "            model = copy.deepcopy(self.model_wrapper)\n",
    "\n",
    "            if adversarial:\n",
    "                X_new1 = X_train.copy()\n",
    "                if X_valid is not None:\n",
    "                    X_new2 = X_valid.copy()\n",
    "                elif X_holdout is not None:\n",
    "                    X_new2 = X_holdout.copy()\n",
    "                X_new = pd.concat([X_new1, X_new2], axis=0)\n",
    "                y_new = np.hstack((np.zeros((X_new1.shape[0])), np.ones((X_new2.shape[0]))))\n",
    "                X_train, X_valid, y_train, y_valid = train_test_split(X_new, y_new)\n",
    "\n",
    "            model.fit(X_train, y_train, X_valid, y_valid, X_hold, y_holdout, params=params)\n",
    "\n",
    "            self.folds_dict[fold_n]['scores'] = model.best_score_\n",
    "            if self.oof.shape[0] != len(X):\n",
    "                self.oof = np.zeros((X.shape[0], self.oof.shape[1]))\n",
    "            if not adversarial:\n",
    "                self.oof[valid_index] = model.predict(X_valid).reshape(-1, n_target)\n",
    "\n",
    "            fold_importance = pd.DataFrame(list(zip(X_train.columns, model.feature_importances_)),\n",
    "                                           columns=['feature', 'importance'])\n",
    "            self.feature_importances = self.feature_importances.append(fold_importance)\n",
    "            self.models.append(model)\n",
    "\n",
    "        self.feature_importances['importance'] = self.feature_importances['importance'].astype(int)\n",
    "\n",
    "        # if params['verbose']:\n",
    "        self.calc_scores_()\n",
    "\n",
    "        if plot:\n",
    "            # print(classification_report(y, self.oof.argmax(1)))\n",
    "            fig, ax = plt.subplots(figsize=(16, 12))\n",
    "            plt.subplot(2, 2, 1)\n",
    "            self.plot_feature_importance(top_n=20)\n",
    "            plt.subplot(2, 2, 2)\n",
    "            self.plot_metric()\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.hist(y.values.reshape(-1, 1) - self.oof)\n",
    "            plt.title('Distribution of errors')\n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.hist(self.oof)\n",
    "            plt.title('Distribution of oof predictions');\n",
    "\n",
    "    def transform_(self, datasets, cols_to_drop):\n",
    "        for name, transformer in self.transformers.items():\n",
    "            transformer.fit(datasets['X_train'], datasets['y_train'])\n",
    "            datasets['X_train'] = transformer.transform(datasets['X_train'])\n",
    "            if datasets['X_valid'] is not None:\n",
    "                datasets['X_valid'] = transformer.transform(datasets['X_valid'])\n",
    "            if datasets['X_holdout'] is not None:\n",
    "                datasets['X_holdout'] = transformer.transform(datasets['X_holdout'])\n",
    "            self.trained_transformers[name].append(transformer)\n",
    "        if cols_to_drop is not None:\n",
    "            cols_to_drop = [col for col in cols_to_drop if col in datasets['X_train'].columns]\n",
    "\n",
    "            datasets['X_train'] = datasets['X_train'].drop(cols_to_drop, axis=1)\n",
    "            if datasets['X_valid'] is not None:\n",
    "                datasets['X_valid'] = datasets['X_valid'].drop(cols_to_drop, axis=1)\n",
    "            if datasets['X_holdout'] is not None:\n",
    "                datasets['X_holdout'] = datasets['X_holdout'].drop(cols_to_drop, axis=1)\n",
    "        self.cols_to_drop = cols_to_drop\n",
    "\n",
    "        return datasets['X_train'], datasets['X_valid'], datasets['X_holdout']\n",
    "\n",
    "    def calc_scores_(self):\n",
    "        print()\n",
    "        datasets = [k for k, v in [v['scores'] for k, v in self.folds_dict.items()][0].items() if len(v) > 0]\n",
    "        self.scores = {}\n",
    "        for d in datasets:\n",
    "            scores = [v['scores'][d][self.eval_metric] for k, v in self.folds_dict.items()]\n",
    "            print(f\"CV mean score on {d}: {np.mean(scores):.4f} +/- {np.std(scores):.4f} std.\")\n",
    "            self.scores[d] = np.mean(scores)\n",
    "\n",
    "    def predict(self, X_test, averaging: str = 'usual'):\n",
    "        \"\"\"\n",
    "        Make prediction\n",
    "\n",
    "        :param X_test:\n",
    "        :param averaging: method of averaging\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        full_prediction = np.zeros((X_test.shape[0], self.oof.shape[1]))\n",
    "        if self.preprocesser is not None:\n",
    "            X_test = self.preprocesser.transform(X_test)\n",
    "        for i in range(len(self.models)):\n",
    "            X_t = X_test.copy()\n",
    "            for name, transformers in self.trained_transformers.items():\n",
    "                X_t = transformers[i].transform(X_t)\n",
    "\n",
    "            if self.cols_to_drop is not None:\n",
    "                cols_to_drop = [col for col in self.cols_to_drop if col in X_t.columns]\n",
    "                X_t = X_t.drop(cols_to_drop, axis=1)\n",
    "            y_pred = self.models[i].predict(X_t[self.folds_dict[i]['columns']]).reshape(-1, full_prediction.shape[1])\n",
    "\n",
    "            # if case transformation changes the number of the rows\n",
    "            if full_prediction.shape[0] != len(y_pred):\n",
    "                full_prediction = np.zeros((y_pred.shape[0], self.oof.shape[1]))\n",
    "\n",
    "            if averaging == 'usual':\n",
    "                full_prediction += y_pred\n",
    "            elif averaging == 'rank':\n",
    "                full_prediction += pd.Series(y_pred).rank().values\n",
    "\n",
    "        return full_prediction / len(self.models)\n",
    "\n",
    "    def plot_feature_importance(self, drop_null_importance: bool = True, top_n: int = 10):\n",
    "        \"\"\"\n",
    "        Plot default feature importance.\n",
    "\n",
    "        :param drop_null_importance: drop columns with null feature importance\n",
    "        :param top_n: show top n columns\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        top_feats = self.get_top_features(drop_null_importance, top_n)\n",
    "        feature_importances = self.feature_importances.loc[self.feature_importances['feature'].isin(top_feats)]\n",
    "        feature_importances['feature'] = feature_importances['feature'].astype(str)\n",
    "        top_feats = [str(i) for i in top_feats]\n",
    "        sns.barplot(data=feature_importances, x='importance', y='feature', orient='h', order=top_feats)\n",
    "        plt.title('Feature importances')\n",
    "        print(feature_importances)\n",
    "\n",
    "    def get_top_features(self, drop_null_importance: bool = True, top_n: int = 10):\n",
    "        \"\"\"\n",
    "        Get top features by importance.\n",
    "\n",
    "        :param drop_null_importance:\n",
    "        :param top_n:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        grouped_feats = self.feature_importances.groupby(['feature'])['importance'].mean()\n",
    "        if drop_null_importance:\n",
    "            grouped_feats = grouped_feats[grouped_feats != 0]\n",
    "        return list(grouped_feats.sort_values(ascending=False).index)[:top_n]\n",
    "\n",
    "    def plot_metric(self):\n",
    "        \"\"\"\n",
    "        Plot training progress.\n",
    "        Inspired by `plot_metric` from https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/plotting.html\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        full_evals_results = pd.DataFrame()\n",
    "        for model in self.models:\n",
    "            evals_result = pd.DataFrame()\n",
    "            for k in model.model.evals_result_.keys():\n",
    "                evals_result[k] = model.model.evals_result_[k][self.eval_metric]\n",
    "            evals_result = evals_result.reset_index().rename(columns={'index': 'iteration'})\n",
    "            full_evals_results = full_evals_results.append(evals_result)\n",
    "\n",
    "        full_evals_results = full_evals_results.melt(id_vars=['iteration']).rename(columns={'value': self.eval_metric,\n",
    "                                                                                            'variable': 'dataset'})\n",
    "        sns.lineplot(data=full_evals_results, x='iteration', y=self.eval_metric, hue='dataset')\n",
    "        plt.title('Training progress')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {'n_estimators':2000,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'subsample': 0.75,\n",
    "            'subsample_freq': 1,\n",
    "            'learning_rate': 0.04,\n",
    "            'feature_fraction': 0.9,\n",
    "         'max_depth': 15,\n",
    "            'lambda_l1': 1,  \n",
    "            'lambda_l2': 1,\n",
    "            'verbose': 100,\n",
    "            'early_stopping_rounds': 100, \n",
    "#             'eval_metric': lgb_kappa_eval,\n",
    "            'eval_metric': 'cappa'\n",
    "            }\n",
    "\n",
    "cat_params = {'iterations':2000,\n",
    "            'loss_function': 'RMSE',\n",
    "            'verbose': 10,\n",
    "            'early_stopping_rounds': 100, \n",
    "#             'task_type':\"GPU\",\n",
    "#             'devices':'0:1',\n",
    "            }\n",
    "para_tune = {'feature_fraction': 0.9985706623844444, 'lambda_l1': 1.3464702887971034, 'lambda_l2': 24.03486401669668, 'max_depth': 9, 'num_leaves': 20, 'subsample': 0.9982720526777418, 'subsample_freq': 2}\n",
    "for k in para_tune:\n",
    "    lgb_params[k] = para_tune[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Wed Jan  1 08:21:17 2020\n",
      "0:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 234ms\tremaining: 7m 48s\n",
      "10:\tlearn: 0.3014725\ttest: 0.3057982\tbest: 0.3057982 (10)\ttotal: 2.56s\tremaining: 7m 43s\n",
      "20:\tlearn: 0.3289608\ttest: 0.3370217\tbest: 0.3370217 (20)\ttotal: 4.86s\tremaining: 7m 37s\n",
      "30:\tlearn: 0.4518421\ttest: 0.4640883\tbest: 0.4640883 (30)\ttotal: 7.11s\tremaining: 7m 31s\n",
      "40:\tlearn: 0.5073477\ttest: 0.5136931\tbest: 0.5136931 (40)\ttotal: 9.32s\tremaining: 7m 25s\n",
      "50:\tlearn: 0.5403683\ttest: 0.5418318\tbest: 0.5418318 (50)\ttotal: 11.6s\tremaining: 7m 21s\n",
      "60:\tlearn: 0.5569497\ttest: 0.5592128\tbest: 0.5592128 (60)\ttotal: 13.8s\tremaining: 7m 18s\n",
      "70:\tlearn: 0.5666205\ttest: 0.5642680\tbest: 0.5648081 (69)\ttotal: 16s\tremaining: 7m 14s\n",
      "80:\tlearn: 0.5752564\ttest: 0.5764790\tbest: 0.5764790 (80)\ttotal: 18.2s\tremaining: 7m 12s\n",
      "90:\tlearn: 0.5830643\ttest: 0.5849775\tbest: 0.5849775 (90)\ttotal: 20.5s\tremaining: 7m 10s\n",
      "100:\tlearn: 0.5904211\ttest: 0.5939415\tbest: 0.5939415 (100)\ttotal: 22.8s\tremaining: 7m 8s\n",
      "110:\tlearn: 0.5928860\ttest: 0.5965615\tbest: 0.5965615 (110)\ttotal: 25.1s\tremaining: 7m 7s\n",
      "120:\tlearn: 0.5981237\ttest: 0.5983577\tbest: 0.5998044 (117)\ttotal: 27.4s\tremaining: 7m 5s\n",
      "130:\tlearn: 0.6013725\ttest: 0.6006866\tbest: 0.6012875 (128)\ttotal: 29.7s\tremaining: 7m 4s\n",
      "140:\tlearn: 0.6048430\ttest: 0.5993212\tbest: 0.6018553 (132)\ttotal: 32.1s\tremaining: 7m 2s\n",
      "150:\tlearn: 0.6053228\ttest: 0.6036358\tbest: 0.6036358 (150)\ttotal: 34.4s\tremaining: 7m 1s\n",
      "160:\tlearn: 0.6065944\ttest: 0.5992060\tbest: 0.6036967 (151)\ttotal: 36.8s\tremaining: 7m\n",
      "170:\tlearn: 0.6088399\ttest: 0.6021966\tbest: 0.6036967 (151)\ttotal: 39.2s\tremaining: 6m 58s\n",
      "180:\tlearn: 0.6103524\ttest: 0.6025774\tbest: 0.6036981 (178)\ttotal: 41.5s\tremaining: 6m 56s\n",
      "190:\tlearn: 0.6126116\ttest: 0.6027843\tbest: 0.6036981 (178)\ttotal: 43.8s\tremaining: 6m 55s\n",
      "200:\tlearn: 0.6147609\ttest: 0.6033494\tbest: 0.6036981 (178)\ttotal: 46.2s\tremaining: 6m 53s\n",
      "210:\tlearn: 0.6149115\ttest: 0.6040356\tbest: 0.6042308 (209)\ttotal: 48.5s\tremaining: 6m 51s\n",
      "220:\tlearn: 0.6165150\ttest: 0.6033837\tbest: 0.6042308 (209)\ttotal: 50.8s\tremaining: 6m 49s\n",
      "230:\tlearn: 0.6169651\ttest: 0.6053735\tbest: 0.6053735 (230)\ttotal: 53.2s\tremaining: 6m 47s\n",
      "240:\tlearn: 0.6176125\ttest: 0.6033109\tbest: 0.6053735 (230)\ttotal: 55.5s\tremaining: 6m 44s\n",
      "250:\tlearn: 0.6173537\ttest: 0.6031275\tbest: 0.6053735 (230)\ttotal: 57.8s\tremaining: 6m 42s\n",
      "260:\tlearn: 0.6186409\ttest: 0.6031371\tbest: 0.6053735 (230)\ttotal: 1m\tremaining: 6m 40s\n",
      "270:\tlearn: 0.6203371\ttest: 0.6030419\tbest: 0.6053735 (230)\ttotal: 1m 2s\tremaining: 6m 38s\n",
      "280:\tlearn: 0.6207737\ttest: 0.6060431\tbest: 0.6060431 (280)\ttotal: 1m 4s\tremaining: 6m 36s\n",
      "290:\tlearn: 0.6207166\ttest: 0.6057222\tbest: 0.6060502 (289)\ttotal: 1m 7s\tremaining: 6m 34s\n",
      "300:\tlearn: 0.6219204\ttest: 0.6063782\tbest: 0.6072495 (295)\ttotal: 1m 9s\tremaining: 6m 32s\n",
      "310:\tlearn: 0.6225730\ttest: 0.6067014\tbest: 0.6073995 (306)\ttotal: 1m 11s\tremaining: 6m 29s\n",
      "320:\tlearn: 0.6224017\ttest: 0.6064245\tbest: 0.6073995 (306)\ttotal: 1m 14s\tremaining: 6m 27s\n",
      "330:\tlearn: 0.6232116\ttest: 0.6062617\tbest: 0.6073995 (306)\ttotal: 1m 16s\tremaining: 6m 25s\n",
      "340:\tlearn: 0.6231892\ttest: 0.6079022\tbest: 0.6079022 (340)\ttotal: 1m 18s\tremaining: 6m 22s\n",
      "350:\tlearn: 0.6234808\ttest: 0.6076477\tbest: 0.6079022 (340)\ttotal: 1m 21s\tremaining: 6m 20s\n",
      "360:\tlearn: 0.6243648\ttest: 0.6064663\tbest: 0.6079022 (340)\ttotal: 1m 23s\tremaining: 6m 18s\n",
      "370:\tlearn: 0.6250601\ttest: 0.6072262\tbest: 0.6079022 (340)\ttotal: 1m 25s\tremaining: 6m 15s\n",
      "380:\tlearn: 0.6257797\ttest: 0.6062674\tbest: 0.6079022 (340)\ttotal: 1m 27s\tremaining: 6m 13s\n",
      "390:\tlearn: 0.6262987\ttest: 0.6060816\tbest: 0.6079022 (340)\ttotal: 1m 30s\tremaining: 6m 11s\n",
      "400:\tlearn: 0.6266387\ttest: 0.6060293\tbest: 0.6079022 (340)\ttotal: 1m 32s\tremaining: 6m 8s\n",
      "410:\tlearn: 0.6277430\ttest: 0.6065866\tbest: 0.6079022 (340)\ttotal: 1m 34s\tremaining: 6m 6s\n",
      "420:\tlearn: 0.6289228\ttest: 0.6064877\tbest: 0.6079022 (340)\ttotal: 1m 37s\tremaining: 6m 4s\n",
      "430:\tlearn: 0.6295392\ttest: 0.6081355\tbest: 0.6081355 (430)\ttotal: 1m 39s\tremaining: 6m 2s\n",
      "440:\tlearn: 0.6303056\ttest: 0.6068169\tbest: 0.6087815 (431)\ttotal: 1m 41s\tremaining: 5m 59s\n",
      "450:\tlearn: 0.6305166\ttest: 0.6062579\tbest: 0.6087815 (431)\ttotal: 1m 44s\tremaining: 5m 57s\n",
      "460:\tlearn: 0.6305014\ttest: 0.6056559\tbest: 0.6087815 (431)\ttotal: 1m 46s\tremaining: 5m 55s\n",
      "470:\tlearn: 0.6305713\ttest: 0.6054015\tbest: 0.6087815 (431)\ttotal: 1m 48s\tremaining: 5m 53s\n",
      "480:\tlearn: 0.6313940\ttest: 0.6053186\tbest: 0.6087815 (431)\ttotal: 1m 51s\tremaining: 5m 50s\n",
      "490:\tlearn: 0.6323637\ttest: 0.6058340\tbest: 0.6087815 (431)\ttotal: 1m 53s\tremaining: 5m 48s\n",
      "500:\tlearn: 0.6327039\ttest: 0.6056143\tbest: 0.6087815 (431)\ttotal: 1m 55s\tremaining: 5m 46s\n",
      "510:\tlearn: 0.6330425\ttest: 0.6054756\tbest: 0.6087815 (431)\ttotal: 1m 58s\tremaining: 5m 43s\n",
      "520:\tlearn: 0.6338954\ttest: 0.6069222\tbest: 0.6087815 (431)\ttotal: 2m\tremaining: 5m 41s\n",
      "530:\tlearn: 0.6342055\ttest: 0.6074250\tbest: 0.6087815 (431)\ttotal: 2m 2s\tremaining: 5m 39s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.6087814808\n",
      "bestIteration = 431\n",
      "\n",
      "Shrink model to first 432 iterations.\n",
      "Fold 2 started at Wed Jan  1 08:23:22 2020\n",
      "0:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 201ms\tremaining: 6m 42s\n",
      "10:\tlearn: 0.3064557\ttest: 0.3135546\tbest: 0.3135546 (10)\ttotal: 2.26s\tremaining: 6m 49s\n",
      "20:\tlearn: 0.3260351\ttest: 0.3304689\tbest: 0.3304689 (20)\ttotal: 4.45s\tremaining: 6m 59s\n",
      "30:\tlearn: 0.4406143\ttest: 0.4349290\tbest: 0.4349290 (30)\ttotal: 6.69s\tremaining: 7m 4s\n",
      "40:\tlearn: 0.5100814\ttest: 0.5085079\tbest: 0.5085079 (40)\ttotal: 8.94s\tremaining: 7m 7s\n",
      "50:\tlearn: 0.5408941\ttest: 0.5313445\tbest: 0.5313445 (50)\ttotal: 11.2s\tremaining: 7m 6s\n",
      "60:\tlearn: 0.5561478\ttest: 0.5518121\tbest: 0.5518121 (60)\ttotal: 13.4s\tremaining: 7m 4s\n",
      "70:\tlearn: 0.5651646\ttest: 0.5662413\tbest: 0.5662413 (70)\ttotal: 15.5s\tremaining: 7m 1s\n",
      "80:\tlearn: 0.5770539\ttest: 0.5706943\tbest: 0.5706943 (80)\ttotal: 17.7s\tremaining: 7m\n",
      "90:\tlearn: 0.5863796\ttest: 0.5804352\tbest: 0.5804352 (90)\ttotal: 20s\tremaining: 6m 59s\n",
      "100:\tlearn: 0.5921597\ttest: 0.5842236\tbest: 0.5842236 (100)\ttotal: 22.3s\tremaining: 6m 58s\n",
      "110:\tlearn: 0.5955421\ttest: 0.5870078\tbest: 0.5874031 (106)\ttotal: 24.5s\tremaining: 6m 56s\n",
      "120:\tlearn: 0.5988996\ttest: 0.5921998\tbest: 0.5921998 (120)\ttotal: 26.7s\tremaining: 6m 54s\n",
      "130:\tlearn: 0.6019862\ttest: 0.5955854\tbest: 0.5955854 (130)\ttotal: 28.9s\tremaining: 6m 52s\n",
      "140:\tlearn: 0.6039531\ttest: 0.5982041\tbest: 0.5982041 (140)\ttotal: 31.1s\tremaining: 6m 49s\n",
      "150:\tlearn: 0.6068953\ttest: 0.5992390\tbest: 0.5998456 (149)\ttotal: 33.3s\tremaining: 6m 48s\n",
      "160:\tlearn: 0.6076794\ttest: 0.6000114\tbest: 0.6002092 (153)\ttotal: 35.6s\tremaining: 6m 46s\n",
      "170:\tlearn: 0.6093266\ttest: 0.5991138\tbest: 0.6011606 (162)\ttotal: 37.8s\tremaining: 6m 43s\n",
      "180:\tlearn: 0.6105079\ttest: 0.5985475\tbest: 0.6011606 (162)\ttotal: 40s\tremaining: 6m 41s\n",
      "190:\tlearn: 0.6126792\ttest: 0.6004408\tbest: 0.6013803 (185)\ttotal: 42.1s\tremaining: 6m 39s\n",
      "200:\tlearn: 0.6134223\ttest: 0.6009940\tbest: 0.6013803 (185)\ttotal: 44.4s\tremaining: 6m 36s\n",
      "210:\tlearn: 0.6144627\ttest: 0.6010164\tbest: 0.6018491 (207)\ttotal: 46.6s\tremaining: 6m 34s\n",
      "220:\tlearn: 0.6164763\ttest: 0.6018790\tbest: 0.6030282 (218)\ttotal: 48.7s\tremaining: 6m 32s\n",
      "230:\tlearn: 0.6177080\ttest: 0.6025605\tbest: 0.6030282 (218)\ttotal: 50.9s\tremaining: 6m 29s\n",
      "240:\tlearn: 0.6181530\ttest: 0.6032183\tbest: 0.6033849 (236)\ttotal: 53.1s\tremaining: 6m 27s\n",
      "250:\tlearn: 0.6192512\ttest: 0.6022586\tbest: 0.6037064 (245)\ttotal: 55.3s\tremaining: 6m 25s\n",
      "260:\tlearn: 0.6187197\ttest: 0.6034402\tbest: 0.6037064 (245)\ttotal: 57.4s\tremaining: 6m 22s\n",
      "270:\tlearn: 0.6205646\ttest: 0.6036971\tbest: 0.6041487 (268)\ttotal: 59.6s\tremaining: 6m 20s\n",
      "280:\tlearn: 0.6209210\ttest: 0.6040141\tbest: 0.6041487 (268)\ttotal: 1m 1s\tremaining: 6m 18s\n",
      "290:\tlearn: 0.6221099\ttest: 0.6049376\tbest: 0.6050005 (289)\ttotal: 1m 3s\tremaining: 6m 15s\n",
      "300:\tlearn: 0.6227504\ttest: 0.6052050\tbest: 0.6057862 (295)\ttotal: 1m 6s\tremaining: 6m 13s\n",
      "310:\tlearn: 0.6228914\ttest: 0.6059226\tbest: 0.6072902 (304)\ttotal: 1m 8s\tremaining: 6m 10s\n",
      "320:\tlearn: 0.6237514\ttest: 0.6063878\tbest: 0.6072902 (304)\ttotal: 1m 10s\tremaining: 6m 8s\n",
      "330:\tlearn: 0.6245377\ttest: 0.6061094\tbest: 0.6072902 (304)\ttotal: 1m 12s\tremaining: 6m 5s\n",
      "340:\tlearn: 0.6255020\ttest: 0.6069763\tbest: 0.6080276 (335)\ttotal: 1m 14s\tremaining: 6m 3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350:\tlearn: 0.6268189\ttest: 0.6055652\tbest: 0.6080276 (335)\ttotal: 1m 16s\tremaining: 6m 1s\n",
      "360:\tlearn: 0.6270134\ttest: 0.6065461\tbest: 0.6080276 (335)\ttotal: 1m 19s\tremaining: 5m 58s\n",
      "370:\tlearn: 0.6275812\ttest: 0.6057796\tbest: 0.6080276 (335)\ttotal: 1m 21s\tremaining: 5m 56s\n",
      "380:\tlearn: 0.6283412\ttest: 0.6063220\tbest: 0.6080276 (335)\ttotal: 1m 23s\tremaining: 5m 54s\n",
      "390:\tlearn: 0.6293357\ttest: 0.6063312\tbest: 0.6080276 (335)\ttotal: 1m 25s\tremaining: 5m 52s\n",
      "400:\tlearn: 0.6299314\ttest: 0.6060244\tbest: 0.6080276 (335)\ttotal: 1m 27s\tremaining: 5m 49s\n",
      "410:\tlearn: 0.6312306\ttest: 0.6053983\tbest: 0.6080276 (335)\ttotal: 1m 29s\tremaining: 5m 47s\n",
      "420:\tlearn: 0.6314263\ttest: 0.6048967\tbest: 0.6080276 (335)\ttotal: 1m 32s\tremaining: 5m 45s\n",
      "430:\tlearn: 0.6320067\ttest: 0.6047865\tbest: 0.6080276 (335)\ttotal: 1m 34s\tremaining: 5m 43s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.6080275653\n",
      "bestIteration = 335\n",
      "\n",
      "Shrink model to first 336 iterations.\n",
      "Fold 3 started at Wed Jan  1 08:24:58 2020\n",
      "0:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 218ms\tremaining: 7m 16s\n",
      "10:\tlearn: 0.3117839\ttest: 0.2954528\tbest: 0.2955373 (9)\ttotal: 2.38s\tremaining: 7m 10s\n",
      "20:\tlearn: 0.3278135\ttest: 0.3062109\tbest: 0.3062109 (20)\ttotal: 4.55s\tremaining: 7m 9s\n",
      "30:\tlearn: 0.4598825\ttest: 0.4439283\tbest: 0.4439283 (30)\ttotal: 6.77s\tremaining: 7m 10s\n",
      "40:\tlearn: 0.5187128\ttest: 0.4952745\tbest: 0.4952745 (40)\ttotal: 8.94s\tremaining: 7m 7s\n",
      "50:\tlearn: 0.5457631\ttest: 0.5277645\tbest: 0.5277645 (50)\ttotal: 11.1s\tremaining: 7m 5s\n",
      "60:\tlearn: 0.5612277\ttest: 0.5430748\tbest: 0.5430748 (60)\ttotal: 13.3s\tremaining: 7m 2s\n",
      "70:\tlearn: 0.5711524\ttest: 0.5531581\tbest: 0.5531581 (70)\ttotal: 15.5s\tremaining: 7m 1s\n",
      "80:\tlearn: 0.5817412\ttest: 0.5609426\tbest: 0.5609426 (80)\ttotal: 17.7s\tremaining: 7m\n",
      "90:\tlearn: 0.5873437\ttest: 0.5661256\tbest: 0.5661256 (90)\ttotal: 20s\tremaining: 7m\n",
      "100:\tlearn: 0.5932611\ttest: 0.5729524\tbest: 0.5729524 (100)\ttotal: 22.4s\tremaining: 7m\n",
      "110:\tlearn: 0.5986998\ttest: 0.5775586\tbest: 0.5778612 (107)\ttotal: 24.7s\tremaining: 7m\n",
      "120:\tlearn: 0.6026372\ttest: 0.5808781\tbest: 0.5808781 (120)\ttotal: 27s\tremaining: 6m 59s\n",
      "130:\tlearn: 0.6064440\ttest: 0.5832328\tbest: 0.5832328 (130)\ttotal: 29.4s\tremaining: 6m 59s\n",
      "140:\tlearn: 0.6099962\ttest: 0.5825518\tbest: 0.5843882 (131)\ttotal: 31.8s\tremaining: 6m 58s\n",
      "150:\tlearn: 0.6122306\ttest: 0.5827008\tbest: 0.5843882 (131)\ttotal: 34.1s\tremaining: 6m 58s\n",
      "160:\tlearn: 0.6148799\ttest: 0.5851116\tbest: 0.5852542 (159)\ttotal: 36.5s\tremaining: 6m 56s\n",
      "170:\tlearn: 0.6168764\ttest: 0.5848417\tbest: 0.5852542 (159)\ttotal: 38.8s\tremaining: 6m 55s\n",
      "180:\tlearn: 0.6192164\ttest: 0.5858115\tbest: 0.5862077 (179)\ttotal: 41.1s\tremaining: 6m 53s\n",
      "190:\tlearn: 0.6207916\ttest: 0.5869884\tbest: 0.5876645 (182)\ttotal: 43.4s\tremaining: 6m 51s\n",
      "200:\tlearn: 0.6223533\ttest: 0.5867549\tbest: 0.5876645 (182)\ttotal: 45.7s\tremaining: 6m 49s\n",
      "210:\tlearn: 0.6236403\ttest: 0.5872191\tbest: 0.5881602 (208)\ttotal: 48s\tremaining: 6m 47s\n",
      "220:\tlearn: 0.6251700\ttest: 0.5870811\tbest: 0.5881602 (208)\ttotal: 50.3s\tremaining: 6m 45s\n",
      "230:\tlearn: 0.6258324\ttest: 0.5888696\tbest: 0.5888696 (230)\ttotal: 52.6s\tremaining: 6m 42s\n",
      "240:\tlearn: 0.6257540\ttest: 0.5876591\tbest: 0.5888696 (230)\ttotal: 54.9s\tremaining: 6m 41s\n",
      "250:\tlearn: 0.6270194\ttest: 0.5879804\tbest: 0.5888696 (230)\ttotal: 57.3s\tremaining: 6m 38s\n",
      "260:\tlearn: 0.6277714\ttest: 0.5864581\tbest: 0.5888696 (230)\ttotal: 59.5s\tremaining: 6m 36s\n",
      "270:\tlearn: 0.6288613\ttest: 0.5859779\tbest: 0.5888696 (230)\ttotal: 1m 1s\tremaining: 6m 34s\n",
      "280:\tlearn: 0.6296543\ttest: 0.5867973\tbest: 0.5888696 (230)\ttotal: 1m 4s\tremaining: 6m 32s\n",
      "290:\tlearn: 0.6302748\ttest: 0.5873866\tbest: 0.5888696 (230)\ttotal: 1m 6s\tremaining: 6m 29s\n",
      "300:\tlearn: 0.6304786\ttest: 0.5868825\tbest: 0.5888696 (230)\ttotal: 1m 8s\tremaining: 6m 27s\n",
      "310:\tlearn: 0.6314973\ttest: 0.5878482\tbest: 0.5888696 (230)\ttotal: 1m 11s\tremaining: 6m 25s\n",
      "320:\tlearn: 0.6320028\ttest: 0.5863927\tbest: 0.5888696 (230)\ttotal: 1m 13s\tremaining: 6m 23s\n",
      "330:\tlearn: 0.6323980\ttest: 0.5867826\tbest: 0.5888696 (230)\ttotal: 1m 15s\tremaining: 6m 20s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.5888695944\n",
      "bestIteration = 230\n",
      "\n",
      "Shrink model to first 231 iterations.\n",
      "Fold 4 started at Wed Jan  1 08:26:15 2020\n",
      "0:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 228ms\tremaining: 7m 34s\n",
      "10:\tlearn: 0.3043638\ttest: 0.2758030\tbest: 0.2763649 (9)\ttotal: 2.57s\tremaining: 7m 44s\n",
      "20:\tlearn: 0.3408835\ttest: 0.3121086\tbest: 0.3121086 (20)\ttotal: 5.03s\tremaining: 7m 53s\n",
      "30:\tlearn: 0.4487674\ttest: 0.4367409\tbest: 0.4367409 (30)\ttotal: 7.43s\tremaining: 7m 52s\n",
      "40:\tlearn: 0.5128737\ttest: 0.4994962\tbest: 0.4994962 (40)\ttotal: 9.91s\tremaining: 7m 53s\n",
      "50:\tlearn: 0.5433191\ttest: 0.5271122\tbest: 0.5271122 (50)\ttotal: 12.3s\tremaining: 7m 51s\n",
      "60:\tlearn: 0.5591150\ttest: 0.5355141\tbest: 0.5367627 (58)\ttotal: 14.8s\tremaining: 7m 49s\n",
      "70:\tlearn: 0.5687996\ttest: 0.5461160\tbest: 0.5461160 (70)\ttotal: 17.2s\tremaining: 7m 48s\n",
      "80:\tlearn: 0.5817875\ttest: 0.5531582\tbest: 0.5538489 (78)\ttotal: 19.6s\tremaining: 7m 45s\n",
      "90:\tlearn: 0.5898228\ttest: 0.5561025\tbest: 0.5561980 (89)\ttotal: 22.1s\tremaining: 7m 43s\n",
      "100:\tlearn: 0.5954496\ttest: 0.5687206\tbest: 0.5687206 (100)\ttotal: 24.6s\tremaining: 7m 42s\n",
      "110:\tlearn: 0.6012998\ttest: 0.5737536\tbest: 0.5737536 (110)\ttotal: 27s\tremaining: 7m 39s\n",
      "120:\tlearn: 0.6050677\ttest: 0.5756434\tbest: 0.5756434 (120)\ttotal: 29.4s\tremaining: 7m 37s\n",
      "130:\tlearn: 0.6087720\ttest: 0.5774641\tbest: 0.5783366 (127)\ttotal: 31.9s\tremaining: 7m 35s\n",
      "140:\tlearn: 0.6119904\ttest: 0.5826261\tbest: 0.5826261 (140)\ttotal: 34.4s\tremaining: 7m 33s\n",
      "150:\tlearn: 0.6135123\ttest: 0.5849266\tbest: 0.5851318 (149)\ttotal: 36.8s\tremaining: 7m 30s\n",
      "160:\tlearn: 0.6167741\ttest: 0.5854646\tbest: 0.5861886 (158)\ttotal: 39.2s\tremaining: 7m 27s\n",
      "170:\tlearn: 0.6193662\ttest: 0.5847265\tbest: 0.5861886 (158)\ttotal: 41.6s\tremaining: 7m 25s\n",
      "180:\tlearn: 0.6223042\ttest: 0.5840494\tbest: 0.5861886 (158)\ttotal: 44s\tremaining: 7m 22s\n",
      "190:\tlearn: 0.6231256\ttest: 0.5858008\tbest: 0.5861949 (187)\ttotal: 46.4s\tremaining: 7m 19s\n",
      "200:\tlearn: 0.6240604\ttest: 0.5854070\tbest: 0.5870411 (193)\ttotal: 48.8s\tremaining: 7m 17s\n",
      "210:\tlearn: 0.6243616\ttest: 0.5869667\tbest: 0.5870411 (193)\ttotal: 51.3s\tremaining: 7m 14s\n",
      "220:\tlearn: 0.6259191\ttest: 0.5863199\tbest: 0.5870411 (193)\ttotal: 53.7s\tremaining: 7m 11s\n",
      "230:\tlearn: 0.6269000\ttest: 0.5847816\tbest: 0.5873239 (222)\ttotal: 56s\tremaining: 7m 9s\n",
      "240:\tlearn: 0.6275803\ttest: 0.5858487\tbest: 0.5873239 (222)\ttotal: 58.4s\tremaining: 7m 6s\n",
      "250:\tlearn: 0.6277268\ttest: 0.5866912\tbest: 0.5875967 (249)\ttotal: 1m\tremaining: 7m 3s\n",
      "260:\tlearn: 0.6284710\ttest: 0.5862023\tbest: 0.5875967 (249)\ttotal: 1m 3s\tremaining: 7m 1s\n",
      "270:\tlearn: 0.6300079\ttest: 0.5875587\tbest: 0.5876716 (266)\ttotal: 1m 5s\tremaining: 6m 58s\n",
      "280:\tlearn: 0.6296689\ttest: 0.5874521\tbest: 0.5880722 (272)\ttotal: 1m 7s\tremaining: 6m 55s\n",
      "290:\tlearn: 0.6305741\ttest: 0.5871075\tbest: 0.5880722 (272)\ttotal: 1m 10s\tremaining: 6m 52s\n",
      "300:\tlearn: 0.6311795\ttest: 0.5857529\tbest: 0.5880722 (272)\ttotal: 1m 12s\tremaining: 6m 49s\n",
      "310:\tlearn: 0.6312918\ttest: 0.5870162\tbest: 0.5880722 (272)\ttotal: 1m 14s\tremaining: 6m 46s\n",
      "320:\tlearn: 0.6314597\ttest: 0.5857964\tbest: 0.5880722 (272)\ttotal: 1m 17s\tremaining: 6m 43s\n",
      "330:\tlearn: 0.6321506\ttest: 0.5857296\tbest: 0.5880722 (272)\ttotal: 1m 19s\tremaining: 6m 40s\n",
      "340:\tlearn: 0.6324058\ttest: 0.5860440\tbest: 0.5880722 (272)\ttotal: 1m 21s\tremaining: 6m 37s\n",
      "350:\tlearn: 0.6323320\ttest: 0.5860345\tbest: 0.5880722 (272)\ttotal: 1m 23s\tremaining: 6m 34s\n",
      "360:\tlearn: 0.6320314\ttest: 0.5859258\tbest: 0.5880722 (272)\ttotal: 1m 26s\tremaining: 6m 31s\n",
      "370:\tlearn: 0.6324068\ttest: 0.5854085\tbest: 0.5880722 (272)\ttotal: 1m 28s\tremaining: 6m 28s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.5880722479\n",
      "bestIteration = 272\n",
      "\n",
      "Shrink model to first 273 iterations.\n",
      "Fold 5 started at Wed Jan  1 08:27:45 2020\n",
      "0:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 224ms\tremaining: 7m 28s\n",
      "10:\tlearn: 0.3107763\ttest: 0.2861559\tbest: 0.2917088 (9)\ttotal: 2.47s\tremaining: 7m 26s\n",
      "20:\tlearn: 0.3446325\ttest: 0.3140852\tbest: 0.3140852 (20)\ttotal: 4.84s\tremaining: 7m 35s\n",
      "30:\tlearn: 0.4672870\ttest: 0.4237006\tbest: 0.4237006 (30)\ttotal: 7.25s\tremaining: 7m 40s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40:\tlearn: 0.5241999\ttest: 0.4858684\tbest: 0.4858684 (40)\ttotal: 9.62s\tremaining: 7m 39s\n",
      "50:\tlearn: 0.5471454\ttest: 0.5061421\tbest: 0.5070935 (49)\ttotal: 12s\tremaining: 7m 38s\n",
      "60:\tlearn: 0.5642502\ttest: 0.5211445\tbest: 0.5211445 (60)\ttotal: 14.4s\tremaining: 7m 37s\n",
      "70:\tlearn: 0.5768999\ttest: 0.5345700\tbest: 0.5345700 (70)\ttotal: 16.8s\tremaining: 7m 36s\n",
      "80:\tlearn: 0.5874234\ttest: 0.5434719\tbest: 0.5434719 (80)\ttotal: 19.2s\tremaining: 7m 35s\n",
      "90:\tlearn: 0.5957483\ttest: 0.5493386\tbest: 0.5493433 (89)\ttotal: 21.6s\tremaining: 7m 32s\n",
      "100:\tlearn: 0.6010213\ttest: 0.5516960\tbest: 0.5516960 (100)\ttotal: 24s\tremaining: 7m 31s\n",
      "110:\tlearn: 0.6059419\ttest: 0.5565484\tbest: 0.5569113 (109)\ttotal: 26.4s\tremaining: 7m 29s\n",
      "120:\tlearn: 0.6104480\ttest: 0.5617318\tbest: 0.5617318 (120)\ttotal: 28.8s\tremaining: 7m 27s\n",
      "130:\tlearn: 0.6149740\ttest: 0.5619935\tbest: 0.5628585 (121)\ttotal: 31.2s\tremaining: 7m 25s\n",
      "140:\tlearn: 0.6165149\ttest: 0.5632380\tbest: 0.5647282 (139)\ttotal: 33.6s\tremaining: 7m 23s\n",
      "150:\tlearn: 0.6193048\ttest: 0.5641459\tbest: 0.5647282 (139)\ttotal: 36s\tremaining: 7m 21s\n",
      "160:\tlearn: 0.6225966\ttest: 0.5651248\tbest: 0.5651248 (160)\ttotal: 38.4s\tremaining: 7m 18s\n",
      "170:\tlearn: 0.6249730\ttest: 0.5649961\tbest: 0.5658374 (161)\ttotal: 40.8s\tremaining: 7m 16s\n",
      "180:\tlearn: 0.6263513\ttest: 0.5630487\tbest: 0.5658374 (161)\ttotal: 43.2s\tremaining: 7m 13s\n",
      "190:\tlearn: 0.6276777\ttest: 0.5626517\tbest: 0.5658374 (161)\ttotal: 45.5s\tremaining: 7m 10s\n",
      "200:\tlearn: 0.6291071\ttest: 0.5647842\tbest: 0.5658374 (161)\ttotal: 47.9s\tremaining: 7m 8s\n",
      "210:\tlearn: 0.6297111\ttest: 0.5636835\tbest: 0.5658374 (161)\ttotal: 50.2s\tremaining: 7m 5s\n",
      "220:\tlearn: 0.6307784\ttest: 0.5643393\tbest: 0.5658374 (161)\ttotal: 52.6s\tremaining: 7m 3s\n",
      "230:\tlearn: 0.6316002\ttest: 0.5640950\tbest: 0.5658374 (161)\ttotal: 54.9s\tremaining: 7m\n",
      "240:\tlearn: 0.6326533\ttest: 0.5639714\tbest: 0.5658374 (161)\ttotal: 57.3s\tremaining: 6m 57s\n",
      "250:\tlearn: 0.6328339\ttest: 0.5630301\tbest: 0.5658374 (161)\ttotal: 59.6s\tremaining: 6m 54s\n",
      "260:\tlearn: 0.6335508\ttest: 0.5649660\tbest: 0.5658374 (161)\ttotal: 1m 1s\tremaining: 6m 52s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.5658374135\n",
      "bestIteration = 161\n",
      "\n",
      "Shrink model to first 162 iterations.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'cappa'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-fdce2d42a7f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mregressor_model1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegressorModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_wrapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCatboostWrapper_regr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m regressor_model1.fit(X=train, y=y, folds=folds, params=cat_params, preprocesser=mt, transformers=transformers,\n\u001b[0;32m---> 14\u001b[0;31m                     eval_metric='cappa', cols_to_drop=cols_to_drop)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-9fb2324b2fe2>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_holdout, y_holdout, folds, params, eval_metric, cols_to_drop, preprocesser, transformers, adversarial, plot)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# if params['verbose']:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_scores_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-9fb2324b2fe2>\u001b[0m in \u001b[0;36mcalc_scores_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_metric\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfolds_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"CV mean score on {d}: {np.mean(scores):.4f} +/- {np.std(scores):.4f} std.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-9fb2324b2fe2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_metric\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfolds_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"CV mean score on {d}: {np.mean(scores):.4f} +/- {np.std(scores):.4f} std.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cappa'"
     ]
    }
   ],
   "source": [
    "y = train['accuracy_group']\n",
    "n_fold = 5\n",
    "folds = GroupKFold(n_splits=n_fold)\n",
    "\n",
    "cat_cols = ['title','world','timestamp_weekday']\n",
    "cols_to_drop = ['installation_id', 'game_session', 'timestamp', 'accuracy_group']\n",
    "lgb_params['cat_cols'] = cat_cols']\n",
    "cat_params['cat_cols'] = cat_cols\n",
    "\n",
    "mt = MainTransformer()\n",
    "ft = FeatureTransformer()\n",
    "transformers = {'ft': ft}\n",
    "regressor_model1 = RegressorModel(model_wrapper=LGBWrapper_regr())\n",
    "regressor_model1.fit(X=train, y=y, folds=folds, params=lgb_params, preprocesser=mt, transformers=transformers,\n",
    "                    eval_metric='cappa', cols_to_drop=cols_to_drop)\n",
    "# regressor_model1 = RegressorModel(model_wrapper=CatboostWrapper_regr())\n",
    "# regressor_model1.fit(X=train, y=y, folds=folds, params=cat_params, preprocesser=mt, transformers=transformers,\n",
    "#                     eval_metric='cappa', cols_to_drop=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整理別人kernel中使用過的特徵\n",
    "\n",
    "feature selection\n",
    "* lasso?\n",
    "* 相對訊息?\n",
    "\n",
    "bayisan optimization參數\n",
    "\n",
    "思考有哪些訓練方式:\n",
    "有沒有比k-fold更好的?\n",
    "semi-supervised learning? 將沒有assessment的使用者label加進訓練資料中\n",
    "k-fold optimize th驗證結果\n",
    "\n",
    "訓練regressor\n",
    "optimize th\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "應該K fold predict reduce train\n",
    "先用Bayesian優化predict kappa\n",
    "再用K fold結果來optimize 邊界\n",
    "\n",
    "用LGB,XGB,Catboost嘗試regressor結果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "The preprocessing is a class, which was initially written by Abhishek Thakur here: https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/76107 and later improved here https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved (the improvement is is speed).\n",
    "\n",
    "It can be used to find optimal coefficients for thresholds. In this kernel I'll show an example, but when you do it, don't forget a proper validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import scipy as sp\n",
    "class OptimizedRounder(object):\n",
    "    \"\"\"\n",
    "    An optimizer for rounding thresholds\n",
    "    to maximize Quadratic Weighted Kappa (QWK) score\n",
    "    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        \"\"\"\n",
    "        Get loss according to\n",
    "        using current coefficients\n",
    "        \n",
    "        :param coef: A list of coefficients that will be used for rounding\n",
    "        :param X: The raw predictions\n",
    "        :param y: The ground truth labels\n",
    "        \"\"\"\n",
    "        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "\n",
    "        return -qwk(y, X_p)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Optimize rounding thresholds\n",
    "        \n",
    "        :param X: The raw predictions\n",
    "        :param y: The ground truth labels\n",
    "        \"\"\"\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        \"\"\"\n",
    "        Make predictions with specified thresholds\n",
    "        \n",
    "        :param X: The raw predictions\n",
    "        :param coef: A list of coefficients that will be used for rounding\n",
    "        \"\"\"\n",
    "        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "\n",
    "\n",
    "    def coefficients(self):\n",
    "        \"\"\"\n",
    "        Return the optimized coefficients\n",
    "        \"\"\"\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr1 = regressor_model1.predict(reduce_train)\n",
    "\n",
    "optR = OptimizedRounder()\n",
    "optR.fit(pr1.reshape(-1,), y)\n",
    "coefficients = optR.coefficients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_preds = optR.predict(pr1.reshape(-1, ), coefficients)\n",
    "qwk(y, opt_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_pred(pr1: np.array, coef: list):\n",
    "    assert len(coef)==3\n",
    "    pr1[pr1 <= coef[0]] = 0\n",
    "    pr1[np.where(np.logical_and(pr1 > coef[0], pr1 <= coef[1]))] = 1\n",
    "    pr1[np.where(np.logical_and(pr1 > coef[1], pr1 <= coef[2]))] = 2\n",
    "    pr1[pr1 > coef[2]] = 3\n",
    "    \n",
    "    return pr1\n",
    "\n",
    "pr1 = cut_pred(pr1,coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(sample_submission, pred, submit_file_name='submission.csv'):\n",
    "    sample_submission['accuracy_group'] = pred.astype(int)\n",
    "    sample_submission.to_csv(submit_file_name, index=False)\n",
    "    print(sample_submission['accuracy_group'].value_counts(normalize=True))\n",
    "    \n",
    "make_submission(sample_submission=sample_submission, pred=pr1, submit_file_name='submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
