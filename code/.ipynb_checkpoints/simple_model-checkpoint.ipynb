{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple model for [Kaggle kernel](https://www.kaggle.com/artgor/quick-and-dirty-regressio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import gc\n",
    "import json\n",
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,psutil\n",
    "\n",
    "process = psutil.Process(os.getpid())\n",
    "print('Used Memory:',process.memory_info().rss / 1024 / 1024,'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import kaggle_util\n",
    "\n",
    "# read data\n",
    "train, test, train_labels, specs, sample_submission = kaggle_util.read_data()\n",
    "# get usefull dict with maping encode\n",
    "train, test, train_labels, win_code, list_of_user_activities, list_of_event_code,\\\n",
    "activities_labels, assess_titles, list_of_event_id, all_title_event_code = kaggle_util.encode_title(train, test, train_labels)\n",
    "# tranform function to get the train and test set\n",
    "reduce_train, reduce_test, categoricals = kaggle_util.get_train_and_test(train, test, win_code, list_of_user_activities, list_of_event_code,\\\n",
    "activities_labels, assess_titles, list_of_event_id, all_title_event_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(reduce_train, reduce_test):\n",
    "    for df in [reduce_train, reduce_test]:\n",
    "        df['installation_session_count'] = df.groupby(['installation_id'])['Clip'].transform('count')\n",
    "        df['installation_duration_mean'] = df.groupby(['installation_id'])['duration_mean'].transform('mean')\n",
    "        #df['installation_duration_std'] = df.groupby(['installation_id'])['duration_mean'].transform('std')\n",
    "        df['installation_title_nunique'] = df.groupby(['installation_id'])['session_title'].transform('nunique')\n",
    "        \n",
    "        df['sum_event_code_count'] = df[[2050, 4100, 4230, 5000, 4235, 2060, 4110, 5010, 2070, 2075, 2080, 2081, 2083, 3110, 4010, 3120, 3121, 4020, 4021, \n",
    "                                        4022, 4025, 4030, 4031, 3010, 4035, 4040, 3020, 3021, 4045, 2000, 4050, 2010, 2020, 4070, 2025, 2030, 4080, 2035, \n",
    "                                        2040, 4090, 4220, 4095]].sum(axis = 1)\n",
    "        \n",
    "        df['installation_event_code_count_mean'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n",
    "        #df['installation_event_code_count_std'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('std')\n",
    "        \n",
    "    features = reduce_train.loc[(reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns # delete useless columns\n",
    "    features = [x for x in features if x not in ['accuracy_group', 'installation_id']] + ['acc_' + title for title in assess_titles]\n",
    "   \n",
    "    return reduce_train, reduce_test, features\n",
    "# call feature engineering function\n",
    "reduce_train, reduce_test, features = preprocess(reduce_train, reduce_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_feature_selection(reduce_train, reduce_test, usefull_features, new_features):\n",
    "    kf = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 42)\n",
    "    target = 'accuracy_group'\n",
    "    oof_pred = np.zeros((len(reduce_train), 4))\n",
    "    for fold, (tr_ind, val_ind) in enumerate(kf.split(reduce_train, reduce_train[target])):\n",
    "        print('Fold {}'.format(fold + 1))\n",
    "        x_train, x_val = reduce_train[usefull_features].iloc[tr_ind], reduce_train[usefull_features].iloc[val_ind]\n",
    "        y_train, y_val = reduce_train[target][tr_ind], reduce_train[target][val_ind]\n",
    "        train_set = lgb.Dataset(x_train, y_train, categorical_feature = categoricals)\n",
    "        val_set = lgb.Dataset(x_val, y_val, categorical_feature = categoricals)\n",
    "\n",
    "        params = {\n",
    "            'learning_rate': 0.01,\n",
    "            'metric': 'multiclass',\n",
    "            'objective': 'multiclass',\n",
    "            'num_classes': 4,\n",
    "            'feature_fraction': 0.7,\n",
    "            'subsample': 0.7,\n",
    "            'n_jobs': -1,\n",
    "            'seed': 50,\n",
    "            'max_depth': 10\n",
    "        }\n",
    "\n",
    "        model = lgb.train(params, train_set, num_boost_round = 100000, early_stopping_rounds = 100, \n",
    "                          valid_sets=[train_set, val_set], verbose_eval = 500)\n",
    "        oof_pred[val_ind] = model.predict(x_val)\n",
    "    # using cohen_kappa because it's the evaluation metric of the competition\n",
    "    loss_score = cohen_kappa_score(reduce_train[target], np.argmax(oof_pred, axis = 1), weights = 'quadratic')\n",
    "    score = loss_score\n",
    "    usefull_new_features = []\n",
    "    for i in new_features:\n",
    "        oof_pred = np.zeros((len(reduce_train), 4))\n",
    "        evaluating_features = usefull_features + usefull_new_features + [i]\n",
    "        print('Evaluating {} column'.format(i))\n",
    "        print('Out best cohen kappa score is : {}'.format(score))\n",
    "        for fold, (tr_ind, val_ind) in enumerate(kf.split(reduce_train, reduce_train[target])):\n",
    "            print('Fold {}'.format(fold + 1))\n",
    "            x_train, x_val = reduce_train[evaluating_features].iloc[tr_ind], reduce_train[evaluating_features].iloc[val_ind]\n",
    "            y_train, y_val = reduce_train[target][tr_ind], reduce_train[target][val_ind]\n",
    "            train_set = lgb.Dataset(x_train, y_train, categorical_feature = categoricals)\n",
    "            val_set = lgb.Dataset(x_val, y_val, categorical_feature = categoricals)\n",
    "\n",
    "            model = lgb.train(params, train_set, num_boost_round = 100000, early_stopping_rounds = 100, \n",
    "                              valid_sets=[train_set, val_set], verbose_eval = 500)\n",
    "            oof_pred[val_ind] = model.predict(x_val)\n",
    "        loss_score = cohen_kappa_score(reduce_train[target], np.argmax(oof_pred, axis = 1), weights = 'quadratic')\n",
    "        print('Our new cohen kappa score is : {}'.format(loss_score))\n",
    "        if loss_score > score:\n",
    "            print('Feature {} is usefull, adding feature to usefull_new_features_list'.format(i))\n",
    "            usefull_new_features.append(i)\n",
    "            score = loss_score\n",
    "        else:\n",
    "            print('Feature {} is useless'.format(i))\n",
    "        gc.collect()\n",
    "    print('The best features are: ', usefull_new_features)\n",
    "    print('Our best cohen kappa score is : ', score)\n",
    "\n",
    "    return usefull_features + usefull_new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lgb(reduce_train, reduce_test, usefull_features):\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle = True, random_state = 42)\n",
    "    target = 'accuracy_group'\n",
    "    oof_pred = np.zeros((len(reduce_train), 4))\n",
    "    y_pred = np.zeros((len(reduce_test), 4))\n",
    "    for fold, (tr_ind, val_ind) in enumerate(kf.split(reduce_train, reduce_train[target])):\n",
    "        print('Fold {}'.format(fold + 1))\n",
    "        x_train, x_val = reduce_train[usefull_features].iloc[tr_ind], reduce_train[usefull_features].iloc[val_ind]\n",
    "        y_train, y_val = reduce_train[target][tr_ind], reduce_train[target][val_ind]\n",
    "        train_set = lgb.Dataset(x_train, y_train, categorical_feature=categoricals)\n",
    "        val_set = lgb.Dataset(x_val, y_val, categorical_feature=categoricals)\n",
    "\n",
    "        params = {\n",
    "            'learning_rate': 0.07,\n",
    "            'metric': 'multiclass',\n",
    "            'objective': 'multiclass',\n",
    "            'num_classes': 4,\n",
    "            'feature_fraction': 0.7,\n",
    "            'subsample': 0.7,\n",
    "            'n_jobs': -1,\n",
    "            'seed': 50,\n",
    "            'max_depth': 10\n",
    "        }\n",
    "\n",
    "        model = lgb.train(params, train_set, num_boost_round = 1000000, early_stopping_rounds = 100, \n",
    "                          valid_sets=[train_set, val_set], verbose_eval = 100)\n",
    "        oof_pred[val_ind] = model.predict(x_val)\n",
    "        y_pred += model.predict(reduce_test[usefull_features]) / 10\n",
    "    loss_score = cohen_kappa_score(reduce_train[target], np.argmax(oof_pred, axis = 1), weights = 'quadratic')\n",
    "    result = pd.Series(np.argmax(oof_pred, axis = 1))\n",
    "    print('Our oof cohen kappa score is: ', loss_score)\n",
    "    print(result.value_counts(normalize = True))\n",
    "    return y_pred\n",
    "\n",
    "def predict(reduce_test, sample_submission, y_pred):\n",
    "    sample_submission['accuracy_group'] = y_pred.argmax(axis = 1)\n",
    "    sample_submission.to_csv('submission.csv', index = False)\n",
    "    print(sample_submission['accuracy_group'].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = reduce_train.loc[(reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns # delete useless columns\n",
    "features = [x for x in features if x not in ['accuracy_group', 'installation_id']]\n",
    "reduce_train[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = run_lgb(reduce_train, reduce_test, features)\n",
    "predict(reduce_test, sample_submission, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators':2000,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'subsample': 0.75,\n",
    "            'subsample_freq': 1,\n",
    "            'learning_rate': 0.04,\n",
    "            'feature_fraction': 0.9,\n",
    "         'max_depth': 15,\n",
    "            'lambda_l1': 1,  \n",
    "            'lambda_l2': 1,\n",
    "            'verbose': 100,\n",
    "            'early_stopping_rounds': 100, 'eval_metric': 'cappa'\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = reduce_train['accuracy_group']\n",
    "n_fold = 5\n",
    "folds = GroupKFold(n_splits=n_fold)\n",
    "cols_to_drop = ['game_session', 'installation_id', 'timestamp', 'accuracy_group', 'timestampDate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = MainTransformer()\n",
    "ft = FeatureTransformer()\n",
    "transformers = {'ft': ft}\n",
    "regressor_model1 = RegressorModel(model_wrapper=LGBWrapper_regr())\n",
    "regressor_model1.fit(X=reduce_train, y=y, folds=folds, params=params, preprocesser=mt, transformers=transformers,\n",
    "                    eval_metric='cappa', cols_to_drop=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import scipy as sp\n",
    "class OptimizedRounder(object):\n",
    "    \"\"\"\n",
    "    An optimizer for rounding thresholds\n",
    "    to maximize Quadratic Weighted Kappa (QWK) score\n",
    "    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        \"\"\"\n",
    "        Get loss according to\n",
    "        using current coefficients\n",
    "        \n",
    "        :param coef: A list of coefficients that will be used for rounding\n",
    "        :param X: The raw predictions\n",
    "        :param y: The ground truth labels\n",
    "        \"\"\"\n",
    "        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "\n",
    "        return -qwk(y, X_p)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Optimize rounding thresholds\n",
    "        \n",
    "        :param X: The raw predictions\n",
    "        :param y: The ground truth labels\n",
    "        \"\"\"\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        \"\"\"\n",
    "        Make predictions with specified thresholds\n",
    "        \n",
    "        :param X: The raw predictions\n",
    "        :param coef: A list of coefficients that will be used for rounding\n",
    "        \"\"\"\n",
    "        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "\n",
    "\n",
    "    def coefficients(self):\n",
    "        \"\"\"\n",
    "        Return the optimized coefficients\n",
    "        \"\"\"\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pr1 = regressor_model1.predict(reduce_train)\n",
    "\n",
    "optR = OptimizedRounder()\n",
    "optR.fit(pr1.reshape(-1,), y)\n",
    "coefficients = optR.coefficients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_preds = optR.predict(pr1.reshape(-1, ), coefficients)\n",
    "qwk(y, opt_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr1 = regressor_model1.predict(reduce_test)\n",
    "pr1[pr1 <= 1.12232214] = 0\n",
    "pr1[np.where(np.logical_and(pr1 > 1.12232214, pr1 <= 1.73925866))] = 1\n",
    "pr1[np.where(np.logical_and(pr1 > 1.73925866, pr1 <= 2.22506454))] = 2\n",
    "pr1[pr1 > 2.22506454] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['accuracy_group'] = pr1.astype(int)\n",
    "sample_submission.to_csv('submission.csv', index=False)\n",
    "sample_submission['accuracy_group'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
