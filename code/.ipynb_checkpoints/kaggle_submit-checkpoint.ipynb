{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_time_th = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "# import timeb\n",
    "from collections import Counter\n",
    "import datetime\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit, RepeatedStratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from bayes_opt import BayesianOptimization\n",
    "import eli5\n",
    "import shap\n",
    "from IPython.display import HTML\n",
    "import json\n",
    "import altair as alt\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from typing import List\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import gc\n",
    "from numba import jit\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "from typing import Any\n",
    "from itertools import product\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "random_seed = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "from time import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython.display import HTML\n",
    "\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import gc\n",
    "import json\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "\n",
    "def read_data():\n",
    "    print('Reading train.csv file....')\n",
    "    train = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv')\n",
    "    print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n",
    "\n",
    "    print('Reading test.csv file....')\n",
    "    test = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')\n",
    "    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n",
    "\n",
    "    print('Reading train_labels.csv file....')\n",
    "    train_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\n",
    "    print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n",
    "\n",
    "    print('Reading specs.csv file....')\n",
    "    specs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')\n",
    "    print('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n",
    "\n",
    "    print('Reading sample_submission.csv file....')\n",
    "    sample_submission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')\n",
    "    print('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n",
    "    return train, test, train_labels, specs, sample_submission\n",
    "\n",
    "\n",
    "\n",
    "def encode_title(train, test, train_labels):\n",
    "    # encode title\n",
    "    # map function:在對一陣列的元素做運算時使用，\n",
    "    \n",
    "    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n",
    "    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n",
    "    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n",
    "    # make a list with all the unique 'titles' from the train and test set\n",
    "    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n",
    "    # make a list with all the unique 'event_code' from the train and test set\n",
    "    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n",
    "    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n",
    "    # make a list with all the unique worlds from the train and test set\n",
    "    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n",
    "    # create a dictionary numerating the titles\n",
    "    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n",
    "    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n",
    "    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n",
    "    activities_world = {'NONE':0, 'MAGMAPEAK':1, 'TREETOPCITY':2, 'CRYSTALCAVES':3}\n",
    "    \n",
    "    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n",
    "    # replace the text titles with the number titles from the dict\n",
    "    train['title'] = train['title'].map(activities_map)\n",
    "    test['title'] = test['title'].map(activities_map)\n",
    "    train['world'] = train['world'].map(activities_world)\n",
    "    test['world'] = test['world'].map(activities_world)\n",
    "    train_labels['title'] = train_labels['title'].map(activities_map)\n",
    "    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n",
    "    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n",
    "    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n",
    "    \n",
    "    # convert text into datetime\n",
    "    train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "    test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "    \n",
    "    train['timestamp_weekday'] = pd.DatetimeIndex(train.timestamp).weekday\n",
    "    train['timestamp_daytime'] = pd.DatetimeIndex(train.timestamp).hour*60 + pd.DatetimeIndex(train.timestamp).minute\n",
    "    test['timestamp_weekday'] = pd.DatetimeIndex(test.timestamp).weekday\n",
    "    test['timestamp_daytime'] = pd.DatetimeIndex(test.timestamp).hour*60 + pd.DatetimeIndex(test.timestamp).minute\n",
    "    \n",
    "    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code, activities_map\n",
    "\n",
    "def get_data_0106(user_sample, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code, activities_map, test_set=False, create_psuedo_assessment=False):\n",
    "    ''' \n",
    "    The user_sample is a DataFrame from train or test where the only one \n",
    "    installation_id is filtered\n",
    "    And the test_set parameter is related with the labels processing, that is only requered\n",
    "    if test_set=False\n",
    "    '''\n",
    "    \n",
    "    ## Set origin parameters\n",
    "    \n",
    "    ## constants and parameters declaration            \n",
    "    all_assessments = []\n",
    "    pseudo_train = []\n",
    "    accumulated_accuracy_group = 0\n",
    "    accumulated_accuracy = -1\n",
    "    accumulated_correct_attempts = 0 \n",
    "    accumulated_incorrect_attempts = 0\n",
    "    accumulated_actions = 0\n",
    "    counter = 0\n",
    "    counter_all_session = 0\n",
    "    \n",
    "    assessment_durations = []\n",
    "    accuracy = -1\n",
    "    \n",
    "    #  User acc history\n",
    "    ## acmu acc each title\n",
    "    acmu_correct_attempt = {'acmu_correct_attempt_' + title: 0 for title in assess_titles}\n",
    "    acmu_incorrect_attempt = {'acmu_incorrect_attempt_' + title: 0 for title in assess_titles}\n",
    "    acmu_accuracy_attempt = {'acmu_accuracy_attempt_' + title: -1 for title in assess_titles}\n",
    "    ## acmu accgp total\n",
    "    accuracy_groups = {'acmu_acc_gp_0':0, 'acmu_acc_gp_1':0, 'acmu_acc_gp_2':0, 'acmu_acc_gp_3':0}\n",
    "    acmu_accuracy_groups_title = {'acmu_accuracy_groups_' + title: [] for title in assess_titles}\n",
    "    ## last acc each title\n",
    "    last_accuracy_title = {'last_acc_' + title: -1 for title in assess_titles}\n",
    "    last2_accuracy_title = {'last2_acc_' + title: -1 for title in assess_titles}\n",
    "    last3_accuracy_title = {'last3_acc_' + title: -1 for title in assess_titles}\n",
    " \n",
    "    # new features: time spent in each activity\n",
    "    last_session_time_sec = 0\n",
    "    \n",
    "    ## learning history\n",
    "    type_count = {'acmu_type_Clip':0, 'acmu_type_Activity': 0, 'acmu_type_Assessment': 0, 'acmu_type_Game':0}\n",
    "    world_count = {'acmu_world_NONE':0, 'acmu_world_MAGMAPEAK': 0, 'acmu_world_TREETOPCITY': 0, 'acmu_world_CRYSTALCAVES':0}\n",
    "    event_code_count = {'acmu_ev_code_'+str(eve): 0 for eve in list_of_event_code}\n",
    "    event_id_count = {'acmu_ev_id_'+str(eve): 0 for eve in list_of_event_id}\n",
    "    title_count = {'acmu_title_'+str(eve): 0 for eve in activities_labels.values()} \n",
    "    title_event_code_count = {'acmu_ev_title_'+str(eve): 0 for eve in all_title_event_code}\n",
    "   \n",
    "    ## timestamp\n",
    "    duration_title = {'duration_title_' + title: [] for title in activities_labels.values()}\n",
    "    duration_title2 = {'duration_title_' + title: 0 for title in activities_labels.values()}\n",
    "\n",
    "    ## media sequence change\n",
    "    media_seq_change = 0\n",
    "    \n",
    "    ## map\n",
    "    world_map = {0:'NONE', 1:'MAGMAPEAK', 2:'TREETOPCITY', 3:'CRYSTALCAVES'}\n",
    "    assessment_to_title ={\"Mushroom Sorter (Assessment)\":['Tree Top City - Level 1', 'Ordering Spheres', 'All Star Sorting', 'Costume Box',\\\n",
    "                                                         'Fireworks (Activity)', '12 Monkeys', 'Tree Top City - Level 2', \\\n",
    "                                                         'Flower Waterer (Activity)', \"Pirate's Tale\", 'Mushroom Sorter (Assessment)'],\n",
    "                            \"Bird Measurer (Assessment)\":['Air Show', 'Treasure Map', 'Tree Top City - Level 3', 'Crystals Rule', 'Rulers',\\\n",
    "                                                          'Bug Measurer (Activity)', 'Bird Measurer (Assessment)'],\n",
    "                          \"Cauldron Filler (Assessment)\":['Magma Peak - Level 1', 'Sandcastle Builder (Activity)', 'Slop Problem',\\\n",
    "                                                          'Scrub-A-Dub', 'Watering Hole (Activity)', 'Magma Peak - Level 2', 'Dino Drink',\\\n",
    "                                                          'Bubble Bath', 'Bottle Filler (Activity)', 'Dino Dive', \\\n",
    "                                                          'Cauldron Filler (Assessment)'],\n",
    "                          \"Cart Balancer (Assessment)\":['Crystal Caves - Level 1', 'Chow Time', 'Balancing Act',\\\n",
    "                                                        'Chicken Balancer (Activity)', 'Lifting Heavy Things', 'Crystal Caves - Level 2',\\\n",
    "                                                        'Honey Cake', 'Happy Camel', 'Cart Balancer (Assessment)'],\n",
    "                          \"Chest Sorter (Assessment)\":['Leaf Leader', 'Crystal Caves - Level 3', 'Heavy, Heavier, Heaviest', 'Pan Balance',\\\n",
    "                                                       'Egg Dropper (Activity)', 'Chest Sorter (Assessment)']}\n",
    "\n",
    "    title_sequence = ['Welcome to Lost Lagoon!', 'Tree Top City - Level 1', 'Ordering Spheres', 'All Star Sorting', 'Costume Box',\\\n",
    "                      'Fireworks (Activity)', '12 Monkeys', 'Tree Top City - Level 2', 'Flower Waterer (Activity)', \"Pirate's Tale\",\\\n",
    "                      'Mushroom Sorter (Assessment)', 'Air Show', 'Treasure Map', 'Tree Top City - Level 3', 'Crystals Rule', 'Rulers',\\\n",
    "                      'Bug Measurer (Activity)', 'Bird Measurer (Assessment)', 'Magma Peak - Level 1', 'Sandcastle Builder (Activity)',\\\n",
    "                      'Slop Problem', 'Scrub-A-Dub', 'Watering Hole (Activity)', 'Magma Peak - Level 2', 'Dino Drink', 'Bubble Bath',\\\n",
    "                      'Bottle Filler (Activity)', 'Dino Dive', 'Cauldron Filler (Assessment)', 'Crystal Caves - Level 1', 'Chow Time',\\\n",
    "                      'Balancing Act', 'Chicken Balancer (Activity)', 'Lifting Heavy Things', 'Crystal Caves - Level 2', 'Honey Cake',\\\n",
    "                      'Happy Camel', 'Cart Balancer (Assessment)', 'Leaf Leader', 'Crystal Caves - Level 3', 'Heavy, Heavier, Heaviest',\\\n",
    "                      'Pan Balance', 'Egg Dropper (Activity)', 'Chest Sorter (Assessment)']\n",
    "    title_sequence = {title:i for i,title in enumerate(title_sequence)}\n",
    "    \n",
    "    session_total_times = 0\n",
    "\n",
    "    last_type = 'None'\n",
    "    last_title = -1\n",
    "    last_assess_title = 'None'\n",
    "    last_title_sequence = 0\n",
    "    reset_cnt = 0\n",
    "    session_timestamp = user_sample['timestamp'].iloc[0]\n",
    "    session_title_sequence = 0\n",
    "    \n",
    "    # itarates through each session of one instalation_id\n",
    "    for i, session in user_sample.groupby('game_session', sort=False):\n",
    "        # i = game_session_id\n",
    "        # session is a DataFrame that contain only one game_session\n",
    "        counter_all_session+=1\n",
    "        \n",
    "        # get some sessions information\n",
    "        session_type = session['type'].iloc[0]\n",
    "        session_title = session['title'].iloc[0]\n",
    "        session_title_text = activities_labels[session_title]\n",
    "        session_world = session['world'].iloc[0]\n",
    "        session_weekday = session['timestamp_weekday'].iloc[0]\n",
    "        session_daytime = session['timestamp_daytime'].iloc[0]\n",
    "        last_session_timestamp_diff = (pd.Timedelta(session['timestamp'].iloc[0] - session_timestamp).seconds)// 60\n",
    "        session_timestamp = session['timestamp'].iloc[0]\n",
    "        \n",
    "        last_title_sequence_diff = title_sequence[session_title_text] - session_title_sequence\n",
    "        media_seq_change += abs(last_title_sequence_diff)\n",
    "        session_title_sequence = title_sequence[session_title_text]\n",
    "        \n",
    "        \n",
    "        ## modified timestamp\n",
    "        ## minus session time when diff time between events is more than 10 min\n",
    "        x = session.timestamp\n",
    "        x = (x - x.shift(1)).astype('timedelta64[s]')\n",
    "        x = np.sum(x[x>5*60])\n",
    "        session_time = (session.iloc[-1, 2] - session.iloc[0, 2] ).seconds - x\n",
    "        \n",
    "        \n",
    "        # for each assessment, and only this kind off session, the features below are processed\n",
    "        # and a register are generated\n",
    "        if (session_type == 'Assessment') & (test_set or len(session)>1):\n",
    "            # search for event_code 4100, that represents the assessments trial\n",
    "            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n",
    "            # then, check the numbers of wins and the number of losses\n",
    "            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n",
    "            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n",
    "            \n",
    "            this_assess_title = assessment_to_title[activities_labels[session_title]]\n",
    "            \n",
    "            # default features\n",
    "            features = dict()\n",
    "            features['installation_id'] = session['installation_id'].iloc[-1]\n",
    "            features['game_session'] = session['game_session'].iloc[-1]\n",
    "            features['title'] = session_title\n",
    "            features['world'] = session_world\n",
    "            features['timestamp'] = session_timestamp\n",
    "            features['timestamp_weekday'] = session_weekday\n",
    "            features['timestamp_daytime'] = session_daytime    \n",
    "            features['last_session_timestamp_diff'] = last_session_timestamp_diff    \n",
    "            \n",
    "            # counting features\n",
    "            features.update(type_count)\n",
    "            features.update(world_count)\n",
    "            features.update(event_code_count)\n",
    "            features.update(event_id_count)\n",
    "            features.update(title_count)\n",
    "            features.update(title_event_code_count)\n",
    "            \n",
    "            # counting features class num\n",
    "            features['title_class_count'] = len(list(filter(lambda x: x > 0, title_count.values())))\n",
    "            features['type_class_count'] = len(list(filter(lambda x: x > 0, type_count.values())))\n",
    "            features['world_class_count'] = len(list(filter(lambda x: x > 0, world_count.values())))\n",
    "            features['event_id_class_count'] = len(list(filter(lambda x: x > 0, event_id_count.values())))\n",
    "            features['event_code_class_count'] = len(list(filter(lambda x: x > 0, event_code_count.values())))\n",
    "            features['event_code_class_count'] = len(list(filter(lambda x: x > 0, title_event_code_count.values())))\n",
    "\n",
    "            # this world\n",
    "            features['this_world_title_count'] = world_count['acmu_world_'+world_map[session_world]]\n",
    "            # this title count\n",
    "            features['this_title_count'] = title_count['acmu_title_'+session_title_text]\n",
    "           \n",
    "            features['this_assessment_title_count'] = 0\n",
    "            for title in this_assess_title:\n",
    "                features['this_assessment_title_count'] += title_count['acmu_title_'+title]\n",
    "            features['this_assessment_title_class_count'] = 0\n",
    "            features['this_assessment_title_class_count_standardize'] = 0\n",
    "            \n",
    "            for title in this_assess_title:\n",
    "                if title_count['acmu_title_'+title]>0:\n",
    "                    features['this_assessment_title_class_count'] += 1\n",
    "            features['this_assessment_title_class_count_standardize'] = features['this_assessment_title_class_count']/len(this_assess_title)\n",
    "\n",
    "            features['last_assessment_title_same'] = session_title_text == last_assess_title\n",
    " \n",
    "            # last acc\n",
    "            features.update(last_accuracy_title)\n",
    "            features.update(last2_accuracy_title)\n",
    "            features.update(last3_accuracy_title)\n",
    "            features['last_acc_this_title'] = last_accuracy_title['last_acc_' + session_title_text]\n",
    "            features['last2_acc_this_title'] = last2_accuracy_title['last2_acc_' + session_title_text]\n",
    "            features['last3_acc_this_title'] = last3_accuracy_title['last3_acc_' + session_title_text]\n",
    "            features['last12_acc_this_titl_same'] = features['last_acc_this_title']==features['last2_acc_this_title']\n",
    "            features['last123_acc_this_titl_same'] = (features['last_acc_this_title']==features['last2_acc_this_title']) &\\\n",
    "                                                    (features['last_acc_this_title']==features['last3_acc_this_title'])\n",
    "            features['last12_acc_this_titl_mean'] = (features['last_acc_this_title']+features['last2_acc_this_title'])/2\n",
    "            features['last123_acc_this_titl_mean'] = (features['last_acc_this_title']+features['last2_acc_this_title']+\\\n",
    "                                                     features['last3_acc_this_title'])/3\n",
    "            \n",
    "            features['last_acc_all'] = accuracy\n",
    "            \n",
    "            # title sequence diff\n",
    "            features['last_title_sequence_diff'] = last_title_sequence_diff\n",
    "            features['media_sequence_change'] = media_seq_change/counter_all_session\n",
    "            \n",
    "            # acmu acc\n",
    "            features.update(accuracy_groups)\n",
    "            features.update(acmu_correct_attempt)\n",
    "            features.update(acmu_incorrect_attempt)\n",
    "            features.update(acmu_accuracy_attempt)\n",
    "            \n",
    "            features.update(duration_title2)\n",
    "            features['this_assessment_title_duration'] = 0\n",
    "            for title in this_assess_title:\n",
    "                features['this_assessment_title_duration'] += duration_title2['duration_title_'+title]\n",
    "\n",
    "            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n",
    "            features['accumulated_incorrect_attempts'] = accumulated_incorrect_attempts\n",
    "            \n",
    "            features['total_times'] = session_total_times            \n",
    "            features['total_event_count'] = sum(event_code_count.values())     \n",
    "            \n",
    "            # the time spent in the app so far\n",
    "            if assessment_durations == []:\n",
    "                features['assessment_duration_mean'] = -1\n",
    "            else:\n",
    "                features['assessment_duration_mean'] = np.mean(assessment_durations)\n",
    "           \n",
    "            # the accurace is the all time wins divided by the all time attempts\n",
    "            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else -1\n",
    "            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else -1\n",
    "            accumulated_accuracy += accuracy\n",
    "            \n",
    "            # a feature of the current accuracy categorized\n",
    "            # it is a counter of how many times this player was in each accuracy group\n",
    "            if accuracy == 0:\n",
    "                features['accuracy_group'] = 0\n",
    "            elif accuracy == 1:\n",
    "                features['accuracy_group'] = 3\n",
    "            elif accuracy == 0.5:\n",
    "                features['accuracy_group'] = 2\n",
    "            else:\n",
    "                features['accuracy_group'] = 1\n",
    "\n",
    "            # mean of the all accuracy groups of this player\n",
    "            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else -1\n",
    "            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n",
    "            features['accumulated_actions'] = accumulated_actions\n",
    "            \n",
    "            acmu_acgp_this = np.mean(acmu_accuracy_groups_title['acmu_accuracy_groups_'+session_title_text]) if \\\n",
    "            len(acmu_accuracy_groups_title['acmu_accuracy_groups_'+session_title_text])>0 else -1\n",
    "            features['acmu_accuracy_group_this_title'] = acmu_acgp_this \n",
    "            \n",
    "            ## update\n",
    "            accumulated_correct_attempts += true_attempts \n",
    "            accumulated_incorrect_attempts += false_attempts\n",
    "            acmu_correct_attempt['acmu_correct_attempt_'+session_title_text] += true_attempts\n",
    "            acmu_incorrect_attempt['acmu_incorrect_attempt_'+session_title_text] += false_attempts\n",
    "            acmu_all = acmu_correct_attempt['acmu_correct_attempt_'+session_title_text] +\\\n",
    "                       acmu_incorrect_attempt['acmu_incorrect_attempt_'+session_title_text] \n",
    "            acmu_accuracy_attempt['acmu_accuracy_attempt_'+session_title_text] = -1 if acmu_all==0 else\\\n",
    "            acmu_correct_attempt['acmu_correct_attempt_'+session_title_text]/acmu_all\n",
    "            \n",
    "            acmu_accuracy_groups_title['acmu_accuracy_groups_'+session_title_text].append(features['accuracy_group'])\n",
    "            \n",
    "            \n",
    "            assessment_durations.append(session_time)\n",
    "            accuracy_groups['acmu_acc_gp_'+str(features['accuracy_group'])] += 1\n",
    "            accumulated_accuracy_group += features['accuracy_group']\n",
    "            last3_accuracy_title['last3_acc_' + session_title_text] = last2_accuracy_title['last2_acc_' + session_title_text]\n",
    "            last2_accuracy_title['last2_acc_' + session_title_text] = last_accuracy_title['last_acc_' + session_title_text]\n",
    "            last_accuracy_title['last_acc_' + session_title_text] = accuracy\n",
    "            \n",
    "            last_assess_title = session_title_text\n",
    "            \n",
    "            # there are some conditions to allow this features to be inserted in the datasets\n",
    "            # if it's a test set, all sessions belong to the final dataset\n",
    "            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n",
    "            # that means, must exist an event_code 4100 or 4110\n",
    "            if test_set:\n",
    "                all_assessments.append(features)\n",
    "            elif true_attempts+false_attempts > 0:\n",
    "                all_assessments.append(features)\n",
    "                \n",
    "            counter += 1\n",
    "        \n",
    "        ## update\n",
    "        # this piece counts how many actions was made in each event_code so far\n",
    "        def update_counters(counter: dict, col: str, perfix: str):\n",
    "                num_of_session_count = Counter(session[col])\n",
    "                for k in num_of_session_count.keys():\n",
    "                    x = k\n",
    "                    if col == 'title':\n",
    "                        x = activities_labels[k]\n",
    "                    counter['acmu_'+perfix+str(x)] += num_of_session_count[k]\n",
    "                return counter\n",
    "        \n",
    "        if last_title!=session_title: \n",
    "            type_count['acmu_type_'+str(session_type)] += 1\n",
    "            world_count['acmu_world_'+world_map[session_world]] += 1   \n",
    "            title_count = update_counters(title_count, 'title',\"title_\")            \n",
    "        last_title = session_title\n",
    "        \n",
    "        event_code_count = update_counters(event_code_count, \"event_code\",\"ev_code_\")\n",
    "        event_id_count = update_counters(event_id_count, \"event_id\",\"ev_id_\")\n",
    "        title_event_code_count = update_counters(title_event_code_count, 'title_event_code',\"ev_title_\")\n",
    "        \n",
    "        duration_title['duration_title_' + activities_labels[session_title]].append(session_time)\n",
    "        duration_title2 = duration_title.copy()\n",
    "        for key in duration_title2:\n",
    "            if len(duration_title2[key])==0:\n",
    "                duration_title2[key] = 0\n",
    "            else:\n",
    "                duration_title2[key] = np.mean(duration_title2[key])\n",
    "        \n",
    "        session_total_times += session_time\n",
    "        # counts how many actions the player has done so far, used in the feature of the same name\n",
    "        accumulated_actions += len(session)\n",
    "        \n",
    "        \n",
    "    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n",
    "    if test_set:\n",
    "        return all_assessments[-1]\n",
    "    # in the train_set, all assessments goes to the dataset\n",
    "    return all_assessments, pseudo_train\n",
    "\n",
    "def get_type_duration(train,type_to_title,activities_labels):\n",
    "    for i in range(type_to_title.shape[0]):\n",
    "        train['duration_type_{}'.format(type_to_title.iloc[i,0])] = 0\n",
    "        for t in type_to_title.iloc[i,1]:        \n",
    "            train['duration_type_{}'.format(type_to_title.iloc[i,0])] += train['duration_title_'+activities_labels[t]]\n",
    "    return train\n",
    "\n",
    "def get_world_duration(train,world_to_title,activities_labels):\n",
    "    for i in range(world_to_title.shape[0]):\n",
    "        train['duration_world_{}'.format(world_to_title.iloc[i,0])] = 0\n",
    "        for t in world_to_title.iloc[i,1]:        \n",
    "            train['duration_world_{}'.format(world_to_title.iloc[i,0])] += train['duration_title_'+activities_labels[t]]\n",
    "    return train\n",
    "\n",
    "def get_this_title_world_duration(train,activities_labels):\n",
    "    train['duration_this_world'] = 0\n",
    "    train['duration_this_title'] = 0\n",
    "#     world_map = {0:'NONE', 1:'MAGMAPEAK', 2:'TREETOPCITY', 3:'CRYSTALCAVES'}\n",
    "    for i in range(train.shape[0]):\n",
    "#         train.loc[i,'duration_this_world'] = train.loc[i,'duration_world_{}'.format(world_map[train.loc[i,'world']])]\n",
    "        train.loc[i,'duration_this_world'] = train.loc[i,'duration_world_{}'.format(train.loc[i,'world'])]\n",
    "        train.loc[i,'duration_this_title'] = train.loc[i,'duration_title_{}'.format(activities_labels[train.loc[i,'title']])]\n",
    "    return train\n",
    "\n",
    "## TODO\n",
    "def get_this_world_type_count(train):\n",
    "    return train\n",
    "\n",
    "## 計算前1~5次session是多久以前(單位:hr)\n",
    "def add_time_diff_hour(reduce_train):\n",
    "    reduce_train['time_diff_hour_session1']=0\n",
    "    reduce_train['time_diff_hour_session2']=0\n",
    "    reduce_train['time_diff_hour_session3']=0\n",
    "    reduce_train['time_diff_hour_session4']=0\n",
    "    reduce_train['time_diff_hour_session5']=0\n",
    "    iid_list = reduce_train['installation_id'].unique()\n",
    "    cnt=0\n",
    "    print('add_time_diff_hour ...')\n",
    "    for iid in iid_list:\n",
    "        cnt+=1\n",
    "        if cnt%300==0:\n",
    "            pass\n",
    "#             print(cnt)\n",
    "        idx = reduce_train['installation_id']==iid\n",
    "        reduce_train.loc[idx,'time_diff_hour_session1'] = reduce_train.loc[idx,'timestamp'].diff(1).fillna(pd.Timedelta('-1 days')).apply(lambda x: x.total_seconds()//60)\n",
    "        reduce_train.loc[idx,'time_diff_hour_session2'] = reduce_train.loc[idx,'timestamp'].diff(2).fillna(pd.Timedelta('-1 days')).apply(lambda x: x.total_seconds()//60)\n",
    "        reduce_train.loc[idx,'time_diff_hour_session3'] = reduce_train.loc[idx,'timestamp'].diff(3).fillna(pd.Timedelta('-1 days')).apply(lambda x: x.total_seconds()//60)\n",
    "        reduce_train.loc[idx,'time_diff_hour_session4'] = reduce_train.loc[idx,'timestamp'].diff(4).fillna(pd.Timedelta('-1 days')).apply(lambda x: x.total_seconds()//60)\n",
    "        reduce_train.loc[idx,'time_diff_hour_session5'] = reduce_train.loc[idx,'timestamp'].diff(5).fillna(pd.Timedelta('-1 days')).apply(lambda x: x.total_seconds()//60)\n",
    "    return reduce_train\n",
    "\n",
    "## 計算前30min~7day有多少個session\n",
    "def add_session_cnt(reduce_train):\n",
    "    iid_list = reduce_train['installation_id'].unique()\n",
    "    reduce_train['session_cnt_10min']=0\n",
    "    reduce_train['session_cnt_30min']=0\n",
    "    reduce_train['session_cnt_1hour']=0\n",
    "    reduce_train['session_cnt_1day']=0\n",
    "    reduce_train['session_cnt_7day']=0\n",
    "    print('add_session_cnt ... ...')\n",
    "    cnt=0\n",
    "    reduce_train2 = reduce_train.copy().set_index('timestamp')\n",
    "    for iid in iid_list:\n",
    "        cnt+=1\n",
    "        if cnt%300==0:\n",
    "            pass\n",
    "#             print(cnt)\n",
    "        idx = reduce_train['installation_id']==iid\n",
    "        idx2 = reduce_train2['installation_id']==iid\n",
    "\n",
    "        reduce_train.loc[idx,'session_cnt_10min'] = reduce_train2.loc[idx2,'title'].rolling('10min').count().values\n",
    "        reduce_train.loc[idx,'session_cnt_30min'] = reduce_train2.loc[idx2,'title'].rolling('30min').count().values\n",
    "        reduce_train.loc[idx,'session_cnt_1hour'] = reduce_train2.loc[idx2,'title'].rolling('60min').count().values\n",
    "        reduce_train.loc[idx,'session_cnt_1day'] = reduce_train2.loc[idx2,'title'].rolling('1D').count().values\n",
    "        reduce_train.loc[idx,'session_cnt_7day'] = reduce_train2.loc[idx2,'title'].rolling('7D').count().values \n",
    "\n",
    "    return reduce_train\n",
    "        \n",
    "\n",
    "def get_train_and_test(train, test, win_code, list_of_user_activities, list_of_event_code,\\\n",
    "activities_labels, assess_titles, list_of_event_id, all_title_event_code, activities_map, create_psuedo_assessment=False):\n",
    "    compiled_train = []\n",
    "    compiled_psuedo_train = []\n",
    "    compiled_test = []\n",
    "    for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort = False)), total =\\\n",
    "                                         train.installation_id.nunique()):\n",
    "        user_data, pseudo_data = get_data_0106(user_sample, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code, activities_map,create_psuedo_assessment=create_psuedo_assessment)\n",
    "        compiled_train += user_data\n",
    "        compiled_psuedo_train += pseudo_data\n",
    "    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), total = test.installation_id.nunique()):\n",
    "        test_data = get_data_0106(user_sample, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code, activities_map, test_set = True)\n",
    "        compiled_test.append(test_data)\n",
    "    reduce_train = pd.DataFrame(compiled_train)\n",
    "    reduce_psuedo_train = pd.DataFrame(compiled_psuedo_train)\n",
    "    reduce_test = pd.DataFrame(compiled_test)\n",
    "    \n",
    "    ## duration_type\n",
    "    type_to_title = train.groupby(['type'])['title'].unique().reset_index()\n",
    "    world_to_title = train.groupby(['world'])['title'].unique().reset_index()\n",
    "    \n",
    "    reduce_train = get_type_duration(reduce_train,type_to_title,activities_labels)\n",
    "    reduce_test = get_type_duration(reduce_test,type_to_title,activities_labels)\n",
    "    \n",
    "    reduce_train = get_world_duration(reduce_train,world_to_title,activities_labels)\n",
    "    reduce_test = get_world_duration(reduce_test,world_to_title,activities_labels)\n",
    "    \n",
    "    reduce_train = get_this_title_world_duration(reduce_train,activities_labels)\n",
    "    reduce_test = get_this_title_world_duration(reduce_test,activities_labels)\n",
    "    \n",
    "    reduce_train = add_time_diff_hour(reduce_train)\n",
    "    reduce_test = add_time_diff_hour(reduce_test)\n",
    "    \n",
    "    reduce_train = add_session_cnt(reduce_train)\n",
    "    reduce_test = add_session_cnt(reduce_test)   \n",
    "    \n",
    "    if create_psuedo_assessment:\n",
    "        reduce_psuedo_train = get_type_duration(reduce_psuedo_train,type_to_title,activities_labels)\n",
    "        reduce_psuedo_train = get_world_duration(reduce_psuedo_train,world_to_title,activities_labels)\n",
    "        reduce_psuedo_train = get_this_title_world_duration(reduce_psuedo_train,activities_labels)\n",
    "        reduce_psuedo_train = add_time_diff_hour(reduce_psuedo_train)\n",
    "        reduce_psuedo_train = add_session_cnt(reduce_psuedo_train)\n",
    "\n",
    "    \n",
    "    ## get \"this\" feature\n",
    "    \n",
    "    \n",
    "    categoricals = ['title']\n",
    "    return reduce_train, reduce_psuedo_train, reduce_test, categoricals\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 刪掉時間太少的gs\n",
    "\n",
    "def del_gs_by_time(train, del_time_th=10):\n",
    "    \"\"\"\n",
    "    del_time_th: delete game_session less than 10s\n",
    "    \"\"\"\n",
    "    print('\\nDeleteing gs by time ... ...')\n",
    "    ## 看兩次game_session時間差異\n",
    "    iid_gp = train.groupby(['installation_id'])\n",
    "    del_gs = []\n",
    "    for iid, gp in iid_gp:\n",
    "        x = gp.groupby(['game_session'],sort=False)['timestamp','type'].first()\n",
    "        x2 = x.timestamp\n",
    "        x2 = (x2 - x2.shift(1)).astype('timedelta64[s]')\n",
    "        del_gs += list(x.index.values[(x2<del_time_th) & (x['type']!='Assessment')])\n",
    "\n",
    "    print(len(del_gs))\n",
    "\n",
    "    ## 直接看game_time\n",
    "    train2 = train[(train['type']=='Activity') | (train['type']=='Game')]\n",
    "    x = train2.groupby(['game_session'])['game_time'].last()\n",
    "    del_gs2 = x[x<del_time_th*1000].index.values\n",
    "    print(len(del_gs2))\n",
    "\n",
    "    del train2\n",
    "    del_gs = set(del_gs)\n",
    "    del_gs.update(set(del_gs2))\n",
    "\n",
    "    print(len(del_gs))\n",
    "    del_id = train.game_session.isin(del_gs)\n",
    "    train2 = train.loc[~del_id]\n",
    "    return train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "train, test, train_labels, specs, sample_submission = read_data()\n",
    "train.timestamp = pd.to_datetime(train.timestamp)\n",
    "test.timestamp = pd.to_datetime(test.timestamp)\n",
    "train = del_gs_by_time(train, del_time_th=del_time_th)\n",
    "test = del_gs_by_time(test, del_time_th=del_time_th)\n",
    "\n",
    "# get usefull dict with maping encode, we implement \"label encoding\" on category features\n",
    "train, test, train_labels, win_code, list_of_user_activities, list_of_event_code,\\\n",
    "activities_labels, assess_titles, list_of_event_id, all_title_event_code, activities_map = encode_title(train, test, train_labels)\n",
    "\n",
    "# tranform function to get the train and test set\n",
    "reduce_train, reduce_psuedo_train, reduce_test, categoricals = get_train_and_test(train, test, win_code, list_of_user_activities, list_of_event_code,\\\n",
    "activities_labels, assess_titles, list_of_event_id, all_title_event_code, activities_map, create_psuedo_assessment=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_pred(pr1: np.array, coef: list):\n",
    "    assert len(coef)==3\n",
    "    pr1[pr1 <= coef[0]] = 0\n",
    "    pr1[np.where(np.logical_and(pr1 > coef[0], pr1 <= coef[1]))] = 1\n",
    "    pr1[np.where(np.logical_and(pr1 > coef[1], pr1 <= coef[2]))] = 2\n",
    "    pr1[pr1 > coef[2]] = 3\n",
    "    \n",
    "    return pr1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "## https://github.com/catboost/tutorials/blob/master/custom_loss/custom_loss_and_metric_tutorial.ipynb\n",
    "class qwk(object):\n",
    "    def get_final_error(self, error, weight):\n",
    "        return error / (weight + 1e-38)\n",
    "\n",
    "    def is_max_optimal(self):\n",
    "        return True\n",
    "\n",
    "    def evaluate(self, approxes, target, weight):\n",
    "        # approxes is list of indexed containers\n",
    "        # (containers with only __len__ and __getitem__ defined), one container\n",
    "        # per approx dimension. Each container contains floats.\n",
    "        # weight is one dimensional indexed container.\n",
    "        # target is float.   \n",
    "        # weight parameter can be None.\n",
    "        # Returns pair (error, weights sum)\n",
    "\n",
    "        assert len(approxes) == 1\n",
    "        assert len(target) == len(approxes[0])\n",
    "\n",
    "        approx = approxes[0]\n",
    "\n",
    "        error_sum = 0.0\n",
    "        weight_sum = 1.0\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for i in range(len(approx)):\n",
    "            y_true.append(target[i])\n",
    "            y_pred.append(approx[i])\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_true = np.array(y_true)\n",
    "        \n",
    "        y_pred[y_pred <= 1.12232214] = 0\n",
    "        y_pred[np.where(np.logical_and(y_pred > 1.12232214, y_pred <= 1.73925866))] = 1\n",
    "        y_pred[np.where(np.logical_and(y_pred > 1.73925866, y_pred <= 2.22506454))] = 2\n",
    "        y_pred[y_pred > 2.22506454] = 3\n",
    "        \n",
    "        error_sum = cohen_kappa_score(y_true, y_pred,weights='quadratic')\n",
    "#         error_sum = qwk(y_true, y_pred)\n",
    "        \n",
    "        return error_sum, weight_sum\n",
    "\n",
    "@jit\n",
    "def qwk_np(a1, a2):\n",
    "    \"\"\"\n",
    "    Source: https://www.kaggle.com/c/data-science-bowl-2019/discussion/114133#latest-660168\n",
    "\n",
    "    :param a1:\n",
    "    :param a2:\n",
    "    :param max_rat:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    max_rat = 3\n",
    "    a1 = np.asarray(a1, dtype=int)\n",
    "    a2 = np.asarray(a2, dtype=int)\n",
    "\n",
    "    hist1 = np.zeros((max_rat + 1, ))\n",
    "    hist2 = np.zeros((max_rat + 1, ))\n",
    "\n",
    "    o = 0\n",
    "    for k in range(a1.shape[0]):\n",
    "        i, j = a1[k], a2[k]\n",
    "        hist1[i] += 1\n",
    "        hist2[j] += 1\n",
    "        o +=  (i - j) * (i - j)\n",
    "\n",
    "    e = 0\n",
    "    for i in range(max_rat + 1):\n",
    "        for j in range(max_rat + 1):\n",
    "            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n",
    "\n",
    "    e = e / a1.shape[0]\n",
    "\n",
    "    return 1 - o / e\n",
    "\n",
    "def eval_qwk_lgb_regr(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Fast cappa eval function for lgb.\n",
    "    \"\"\"\n",
    "    y_pred2 = y_pred.copy()\n",
    "    y_pred2[y_pred2 <= 1.12232214] = 0\n",
    "    y_pred2[np.where(np.logical_and(y_pred2 > 1.12232214, y_pred2 <= 1.73925866))] = 1\n",
    "    y_pred2[np.where(np.logical_and(y_pred2 > 1.73925866, y_pred2 <= 2.22506454))] = 2\n",
    "    y_pred2[y_pred2 > 2.22506454] = 3\n",
    "\n",
    "    # y_pred = y_pred.reshape(len(np.unique(y_true)), -1).argmax(axis=0)\n",
    "\n",
    "    return 'qwk', qwk_np(y_true, y_pred2), True\n",
    "\n",
    "def eval_qwk_lgb_regr2(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Fast cappa eval function for lgb.\n",
    "    \"\"\"\n",
    "    y_pred2 = y_pred.copy()\n",
    "    y_pred2[y_pred2 <= 0.5] = 0\n",
    "    y_pred2[np.where(np.logical_and(y_pred2 > 0.5, y_pred2 <= 1.5))] = 1\n",
    "    y_pred2[np.where(np.logical_and(y_pred2 > 1.5, y_pred2 <= 2.5))] = 2\n",
    "    y_pred2[y_pred2 > 2.5] = 3\n",
    "\n",
    "    # y_pred = y_pred.reshape(len(np.unique(y_true)), -1).argmax(axis=0)\n",
    "\n",
    "    return 'qwk2', qwk_np(y_true, y_pred2), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_col(df, cols):\n",
    "    ## return the index of cols in df.columns, ignore if col not exit\n",
    "    cols2 = []\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            cols2.append(c)\n",
    "    return cols2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "## Wrapper\n",
    "class LGBWrapper_regr(object):\n",
    "    \"\"\"\n",
    "    A wrapper for lightgbm model so that we will have a single api for various models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = lgb.LGBMRegressor()\n",
    "\n",
    "    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None, y_holdout=None, params=None, permutation=False):\n",
    "        if 'cat_cols' in params.keys():\n",
    "            cat_cols = [col for col in params['cat_cols'] if col in X_train.columns]\n",
    "            if len(cat_cols) > 0:\n",
    "                categorical_columns = params['cat_cols']\n",
    "            else:\n",
    "                categorical_columns = 'auto'\n",
    "            del params['cat_cols']\n",
    "        else:\n",
    "            categorical_columns = 'auto'\n",
    "        \n",
    "#         eval_metric = lambda y_true, y_pred: [eval_qwk_lgb_regr(y_true, y_pred), eval_qwk_lgb_regr2(y_true, y_pred)]\n",
    "        eval_metric = eval_qwk_lgb_regr\n",
    "\n",
    "        eval_set = [(X_train, y_train)]\n",
    "        eval_names = ['train']\n",
    "        self.model = self.model.set_params(**params)\n",
    "\n",
    "        if X_valid is not None:\n",
    "            eval_set.append((X_valid, y_valid))\n",
    "            eval_names.append('valid')\n",
    "\n",
    "        if X_holdout is not None:\n",
    "            eval_set.append((X_holdout, y_holdout))\n",
    "            eval_names.append('holdout')\n",
    "\n",
    "        self.model.fit(X=X_train, y=y_train,\n",
    "                       eval_set=eval_set, eval_names=eval_names, eval_metric=eval_metric,\n",
    "                       verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'],\n",
    "                       categorical_feature=categorical_columns)\n",
    "\n",
    "        self.best_score_ = self.model.best_score_\n",
    "        self.feature_importances_ = self.model.feature_importances_\n",
    "        \n",
    "        if permutation:\n",
    "#             perm = PermutationImportance(self.model, random_state=self.random_seed).fit(X_valid, y_valid)\n",
    "            perm = PermutationImportance(self.model).fit(X_valid, y_valid)\n",
    "            self.feature_importances2_ = pd.DataFrame({'feature':X_valid.columns.tolist(),'importance':perm.feature_importances_})\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test, num_iteration=self.model.best_iteration_)\n",
    "\n",
    "\n",
    "class CatboostWrapper_regr(object):\n",
    "    \"\"\"\n",
    "    A wrapper for catboost model so that we will have a single api for various models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = CatBoostRegressor()\n",
    "\n",
    "    def fit(self, X_train, y_train, X_valid=None, y_valid=None, X_holdout=None, y_holdout=None, params=None, permutation=False):\n",
    "        \n",
    "        if 'cat_cols' in params.keys():\n",
    "            cat_cols = [col for col in params['cat_cols'] if col in X_train.columns]\n",
    "            if len(cat_cols) > 0:\n",
    "                categorical_columns = params['cat_cols']\n",
    "            else:\n",
    "                categorical_columns = None\n",
    "            del params['cat_cols']\n",
    "        else:\n",
    "            categorical_columns = None\n",
    "\n",
    "        eval_set = []\n",
    "        eval_names = []\n",
    "        self.model = self.model.set_params(**params)\n",
    "        self.model = self.model.set_params(eval_metric=qwk())\n",
    "        \n",
    "        \n",
    "        if X_valid is not None:\n",
    "            eval_set.append((X_valid, y_valid))\n",
    "            eval_names.append('valid')\n",
    "        elif X_holdout is not None:\n",
    "            eval_set.append((X_holdout, y_holdout))\n",
    "            eval_names.append('holdout')\n",
    "        else:\n",
    "            eval_set.append((X_train, y_train))\n",
    "            eval_names.append('train')\n",
    "\n",
    "        self.model.fit(X=X_train, y=y_train,\n",
    "                       eval_set=eval_set,\n",
    "                       verbose=params['verbose'], early_stopping_rounds=params['early_stopping_rounds'],\n",
    "                       cat_features=categorical_columns)\n",
    "\n",
    "        self.best_score_ = self.model.get_best_score()\n",
    "        \n",
    "        test_pool=Pool(X_valid, y_valid,cat_features=categorical_columns)\n",
    "        self.feature_importances_ = self.model.get_feature_importance(test_pool,thread_count=4)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test, ntree_end=self.model.get_best_iteration())\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class MainTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, convert_cyclical: bool = False, create_interactions: bool = False, n_interactions: int = 20):\n",
    "        \"\"\"\n",
    "        Main transformer for the data. Can be used for processing on the whole data.\n",
    "\n",
    "        :param convert_cyclical: convert cyclical features into continuous\n",
    "        :param create_interactions: create interactions between features\n",
    "        \"\"\"\n",
    "\n",
    "        self.convert_cyclical = convert_cyclical\n",
    "        self.create_interactions = create_interactions\n",
    "        self.feats_for_interaction = None\n",
    "        self.n_interactions = n_interactions\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        if self.create_interactions:\n",
    "            self.feats_for_interaction = [col for col in X.columns if 'sum' in col\n",
    "                                          or 'mean' in col or 'max' in col or 'std' in col\n",
    "                                          or 'attempt' in col]\n",
    "            self.feats_for_interaction1 = np.random.choice(self.feats_for_interaction, self.n_interactions)\n",
    "            self.feats_for_interaction2 = np.random.choice(self.feats_for_interaction, self.n_interactions)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        data = copy.deepcopy(X)\n",
    "        if self.create_interactions:\n",
    "            for col1 in self.feats_for_interaction1:\n",
    "                for col2 in self.feats_for_interaction2:\n",
    "                    data[f'{col1}_int_{col2}'] = data[col1] * data[col2]\n",
    "\n",
    "        if self.convert_cyclical:\n",
    "            data['timestampHour'] = np.sin(2 * np.pi * data['timestampHour'] / 23.0)\n",
    "            data['timestampMonth'] = np.sin(2 * np.pi * data['timestampMonth'] / 23.0)\n",
    "            data['timestampWeek'] = np.sin(2 * np.pi * data['timestampWeek'] / 23.0)\n",
    "            data['timestampMinute'] = np.sin(2 * np.pi * data['timestampMinute'] / 23.0)\n",
    "\n",
    "#         data['installation_session_count'] = data.groupby(['installation_id'])['Clip'].transform('count')\n",
    "#         data['installation_duration_mean'] = data.groupby(['installation_id'])['duration_mean'].transform('mean')\n",
    "#         data['installation_title_nunique'] = data.groupby(['installation_id'])['session_title'].transform('nunique')\n",
    "\n",
    "#         data['sum_event_code_count'] = data[['2000', '3010', '3110', '4070', '4090', '4030', '4035', '4021', '4020', '4010', '2080', '2083', '2040', '2020', '2030', '3021', '3121', '2050', '3020', '3120', '2060', '2070', '4031', '4025', '5000', '5010', '2081', '2025', '4022', '2035', '4040', '4100', '2010', '4110', '4045', '4095', '4220', '2075', '4230', '4235', '4080', '4050']].sum(axis=1)\n",
    "\n",
    "        # data['installation_event_code_count_mean'] = data.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n",
    "\n",
    "        return data\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        data = copy.deepcopy(X)\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "\n",
    "\n",
    "class FeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, main_cat_features: list = None, num_cols: list = None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param main_cat_features:\n",
    "        :param num_cols:\n",
    "        \"\"\"\n",
    "        self.main_cat_features = main_cat_features\n",
    "        self.num_cols = num_cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "#         self.num_cols = [col for col in X.columns if 'sum' in col or 'mean' in col or 'max' in col or 'std' in col\n",
    "#                          or 'attempt' in col]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        data = copy.deepcopy(X)\n",
    "#         for col in self.num_cols:\n",
    "#             data[f'{col}_to_mean'] = data[col] / data.groupby('installation_id')[col].transform('mean')\n",
    "#             data[f'{col}_to_std'] = data[col] / data.groupby('installation_id')[col].transform('std')\n",
    "\n",
    "        return data\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        data = copy.deepcopy(X)\n",
    "        self.fit(data)\n",
    "        return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class RegressorModel(object):\n",
    "    \"\"\"\n",
    "    A wrapper class for regression models.\n",
    "    It can be used for training and prediction.\n",
    "    Can plot feature importance and training progress (if relevant for model).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, columns: list=None, model_wrapper=None, random_state=33):\n",
    "        \"\"\"\n",
    "        :param columns: columns to train\n",
    "        :param model_wrapper:\n",
    "        \"\"\"\n",
    "        self.columns = columns\n",
    "        self.model_wrapper = model_wrapper\n",
    "        self.random_seed = random_state\n",
    "        \n",
    "        self.result_dict = {}\n",
    "        self.train_one_fold = False\n",
    "        self.preprocesser = None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y,\n",
    "            X_holdout: pd.DataFrame = None, y_holdout=None,\n",
    "            X_append: pd.DataFrame = None, y_append: pd.DataFrame = None,\n",
    "            folds=None,\n",
    "            params: dict = None,\n",
    "            eval_metric='rmse',\n",
    "            cols_to_drop: list = None,\n",
    "            preprocesser=None,\n",
    "            transformers: dict = None,\n",
    "            adversarial: bool = False,\n",
    "            plot: bool = True,\n",
    "            permutation=False):\n",
    "        \"\"\"\n",
    "        Training the model.\n",
    "\n",
    "        :param X: training data\n",
    "        :param y: training target\n",
    "        :param X_holdout: holdout data\n",
    "        :param y_holdout: holdout target\n",
    "        :param folds: folds to split the data. If not defined, then model will be trained on the whole X\n",
    "        :param params: training parameters\n",
    "        :param eval_metric: metric for validataion\n",
    "        :param cols_to_drop: list of columns to drop (for example ID)\n",
    "        :param preprocesser: preprocesser class\n",
    "        :param transformers: transformer to use on folds\n",
    "        :param adversarial\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        if folds is None:\n",
    "            folds = KFold(n_splits=3, random_state=self.random_seed)\n",
    "            self.train_one_fold = True\n",
    "\n",
    "        self.columns = X.columns if self.columns is None else self.columns\n",
    "        ## model FI and permutation FI\n",
    "        self.feature_importances = pd.DataFrame(columns=['feature', 'importance'])\n",
    "        self.feature_importances_p = pd.DataFrame(columns=['feature', 'importance'])\n",
    "        self.trained_transformers = {k: [] for k in transformers}\n",
    "        self.transformers = transformers\n",
    "        self.models = []\n",
    "        self.folds_dict = {}\n",
    "        self.eval_metric = eval_metric\n",
    "        n_target = 1\n",
    "        self.oof = np.zeros((len(X), n_target))\n",
    "        self.n_target = n_target\n",
    "\n",
    "        X = X[self.columns]\n",
    "        if X_holdout is not None:\n",
    "            X_holdout = X_holdout[self.columns]\n",
    "        if X_append is not None:\n",
    "            X_append = X_append[self.columns]\n",
    "\n",
    "        if preprocesser is not None:\n",
    "            self.preprocesser = preprocesser\n",
    "            self.preprocesser.fit(X, y)\n",
    "            X = self.preprocesser.transform(X, y)\n",
    "            self.columns = X.columns.tolist()\n",
    "            if X_holdout is not None:\n",
    "                X_holdout = self.preprocesser.transform(X_holdout)\n",
    "\n",
    "        for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y, X['installation_id'])):\n",
    "\n",
    "            if X_holdout is not None:\n",
    "                X_hold = X_holdout.copy()\n",
    "            else:\n",
    "                X_hold = None\n",
    "                \n",
    "            print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
    "            self.folds_dict[fold_n] = {}\n",
    "\n",
    "            X_train, X_valid = pd.concat([X.iloc[train_index],X_append],axis=0), X.iloc[valid_index]\n",
    "            y_train, y_valid = pd.concat([y.iloc[train_index],y_append],axis=0), y.iloc[valid_index]\n",
    "            if self.train_one_fold:\n",
    "                X_train = X[self.original_columns]\n",
    "                y_train = y\n",
    "                X_valid = None\n",
    "                y_valid = None\n",
    "\n",
    "            datasets = {'X_train': X_train, 'X_valid': X_valid, 'X_holdout': X_hold, 'y_train': y_train}\n",
    "            X_train, X_valid, X_hold = self.transform_(datasets, cols_to_drop)\n",
    "\n",
    "            self.folds_dict[fold_n]['columns'] = X_train.columns.tolist()\n",
    "\n",
    "            model = copy.deepcopy(self.model_wrapper)\n",
    "\n",
    "#             if adversarial:\n",
    "#                 X_new1 = X_train.copy()\n",
    "#                 if X_valid is not None:\n",
    "#                     X_new2 = X_valid.copy()\n",
    "#                 elif X_holdout is not None:\n",
    "#                     X_new2 = X_holdout.copy()\n",
    "#                 X_new = pd.concat([X_new1, X_new2], axis=0)\n",
    "#                 y_new = np.hstack((np.zeros((X_new1.shape[0])), np.ones((X_new2.shape[0]))))\n",
    "#                 X_train, X_valid, y_train, y_valid = train_test_split(X_new, y_new)\n",
    "\n",
    "            model.fit(X_train, y_train, X_valid, y_valid, X_hold, y_holdout, params=params.copy(), permutation=permutation)\n",
    "            self.folds_dict[fold_n]['scores'] = model.best_score_\n",
    "            \n",
    "            if self.oof.shape[0] != len(X):\n",
    "                self.oof = np.zeros((X.shape[0], self.oof.shape[1]))\n",
    "            if not adversarial:\n",
    "                self.oof[valid_index] = model.predict(X_valid).reshape(-1, n_target)\n",
    "\n",
    "            fold_importance = pd.DataFrame(list(zip(X_train.columns, model.feature_importances_)),\n",
    "                                           columns=['feature', 'importance'])\n",
    "            self.feature_importances = self.feature_importances.append(fold_importance)\n",
    "            if permutation:\n",
    "                fold_importance_p = pd.DataFrame(list(zip(X_train.columns, model.feature_importances2_)),\n",
    "                                               columns=['feature', 'importance'])\n",
    "                self.feature_importances_p = self.feature_importances.append(fold_importance_p)\n",
    "            \n",
    "            self.models.append(model)\n",
    "\n",
    "        self.feature_importances['importance'] = self.feature_importances['importance'].astype(float)\n",
    "        if permutation:\n",
    "            self.feature_importances_p['importance'] = self.feature_importances_p['importance'].astype(float)\n",
    "\n",
    "        self.calc_scores_()\n",
    "        \n",
    "        if plot:\n",
    "            fig, ax = plt.subplots(figsize=(16, 12))\n",
    "            plt.subplot(2, 2, 1)\n",
    "            self.plot_feature_importance(top_n=20)\n",
    "            plt.subplot(2, 2, 2)\n",
    "            self.plot_metric()\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.hist(y.values.reshape(-1, 1) - self.oof)\n",
    "            plt.title('Distribution of errors')\n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.hist(self.oof)\n",
    "            plt.title('Distribution of oof predictions');\n",
    "\n",
    "    def transform_(self, datasets, cols_to_drop):\n",
    "        for name, transformer in self.transformers.items():\n",
    "            transformer.fit(datasets['X_train'], datasets['y_train'])\n",
    "            datasets['X_train'] = transformer.transform(datasets['X_train'])\n",
    "            if datasets['X_valid'] is not None:\n",
    "                datasets['X_valid'] = transformer.transform(datasets['X_valid'])\n",
    "            if datasets['X_holdout'] is not None:\n",
    "                datasets['X_holdout'] = transformer.transform(datasets['X_holdout'])\n",
    "            self.trained_transformers[name].append(transformer)\n",
    "        if cols_to_drop is not None:\n",
    "            cols_to_drop = [col for col in cols_to_drop if col in datasets['X_train'].columns]\n",
    "\n",
    "            datasets['X_train'] = datasets['X_train'].drop(cols_to_drop, axis=1)\n",
    "            if datasets['X_valid'] is not None:\n",
    "                datasets['X_valid'] = datasets['X_valid'].drop(cols_to_drop, axis=1)\n",
    "            if datasets['X_holdout'] is not None:\n",
    "                datasets['X_holdout'] = datasets['X_holdout'].drop(cols_to_drop, axis=1)\n",
    "        self.cols_to_drop = cols_to_drop\n",
    "\n",
    "        return datasets['X_train'], datasets['X_valid'], datasets['X_holdout']\n",
    "\n",
    "    def calc_scores_(self):\n",
    "        print()\n",
    "        datasets = [k for k, v in [v['scores'] for k, v in self.folds_dict.items()][0].items() if len(v) > 0]\n",
    "        self.scores = {}\n",
    "        for d in datasets:\n",
    "            scores = [v['scores'][d][self.eval_metric] for k, v in self.folds_dict.items()]\n",
    "            print(f\"CV mean score on {d}: {np.mean(scores):.4f} +/- {np.std(scores):.4f} std.\")\n",
    "            self.scores[d] = np.mean(scores)\n",
    "\n",
    "    def predict(self, X_test, averaging: str = 'usual'):\n",
    "        \"\"\"\n",
    "        Make prediction\n",
    "\n",
    "        :param X_test:\n",
    "        :param averaging: method of averaging\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        full_prediction = np.zeros((X_test.shape[0], self.oof.shape[1]))\n",
    "        if self.preprocesser is not None:\n",
    "            X_test = self.preprocesser.transform(X_test)\n",
    "        for i in range(len(self.models)):\n",
    "            X_t = X_test.copy()\n",
    "            for name, transformers in self.trained_transformers.items():\n",
    "                X_t = transformers[i].transform(X_t)\n",
    "\n",
    "            if self.cols_to_drop is not None:\n",
    "                cols_to_drop = [col for col in self.cols_to_drop if col in X_t.columns]\n",
    "                X_t = X_t.drop(cols_to_drop, axis=1)\n",
    "            y_pred = self.models[i].predict(X_t[self.folds_dict[i]['columns']]).reshape(-1, full_prediction.shape[1])\n",
    "\n",
    "            # if case transformation changes the number of the rows\n",
    "            if full_prediction.shape[0] != len(y_pred):\n",
    "                full_prediction = np.zeros((y_pred.shape[0], self.oof.shape[1]))\n",
    "\n",
    "            if averaging == 'usual':\n",
    "                full_prediction += y_pred\n",
    "            elif averaging == 'rank':\n",
    "                full_prediction += pd.Series(y_pred).rank().values\n",
    "\n",
    "        return full_prediction / len(self.models)\n",
    "\n",
    "    def plot_feature_importance(self, drop_null_importance: bool = True, top_n: int = 10):\n",
    "        \"\"\"\n",
    "        Plot default feature importance.\n",
    "\n",
    "        :param drop_null_importance: drop columns with null feature importance\n",
    "        :param top_n: show top n columns\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        top_feats = self.get_top_features(drop_null_importance, top_n)\n",
    "        \n",
    "        feature_importances = self.feature_importances.loc[self.feature_importances['feature'].isin(top_feats)]\n",
    "        feature_importances['feature'] = feature_importances['feature'].astype(str)\n",
    "        top_feats = [str(i) for i in top_feats]\n",
    "        sns.barplot(data=feature_importances, x='importance', y='feature', orient='h', order=top_feats)\n",
    "        plt.title('Feature importances')\n",
    "        print(feature_importances.sort_values(by='importance'))\n",
    "\n",
    "    def get_top_features(self, drop_null_importance: bool = True, top_n: int = 10, fi_type=1):\n",
    "        \"\"\"\n",
    "        Get top features by importance.\n",
    "\n",
    "        :param drop_null_importance:\n",
    "        :param top_n:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        fi = self.feature_importances\n",
    "        if fi_type=='p':\n",
    "            fi = self.feature_importances_p\n",
    "            \n",
    "        grouped_feats = fi.groupby(['feature'])['importance'].mean()\n",
    "        if drop_null_importance:\n",
    "            grouped_feats = grouped_feats[grouped_feats != 0]\n",
    "        return list(grouped_feats.sort_values(ascending=False).index)[:top_n]\n",
    "\n",
    "    def get_top_features2(self, drop_null_importance: bool = True, top_n: int = 10, fi_type=1):\n",
    "        \"\"\"\n",
    "        Get top features by importance.\n",
    "\n",
    "        :param drop_null_importance:\n",
    "        :param top_n:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        fi = self.feature_importances\n",
    "        if fi_type=='p':\n",
    "            fi = self.feature_importances_p\n",
    "            \n",
    "        grouped_feats = fi.groupby(['feature'])['importance'].mean()\n",
    "        if drop_null_importance:\n",
    "            grouped_feats = grouped_feats[grouped_feats != 0]\n",
    "        return grouped_feats.sort_values(ascending=False).iloc[:top_n]\n",
    "\n",
    "    def plot_metric(self):\n",
    "        \"\"\"\n",
    "        Plot training progress.\n",
    "        Inspired by `plot_metric` from https://lightgbm.readthedocs.io/en/latest/_modules/lightgbm/plotting.html\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        full_evals_results = pd.DataFrame()\n",
    "        for model in self.models:\n",
    "            evals_result = pd.DataFrame()\n",
    "            for k in model.model.evals_result_.keys():\n",
    "                evals_result[k] = model.model.evals_result_[k][self.eval_metric]\n",
    "            evals_result = evals_result.reset_index().rename(columns={'index': 'iteration'})\n",
    "            full_evals_results = full_evals_results.append(evals_result)\n",
    "\n",
    "        full_evals_results = full_evals_results.melt(id_vars=['iteration']).rename(columns={'value': self.eval_metric,\n",
    "                                                                                            'variable': 'dataset'})\n",
    "        sns.lineplot(data=full_evals_results, x='iteration', y=self.eval_metric, hue='dataset')\n",
    "        plt.title('Training progress')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {'n_estimators':2000,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'subsample': 0.75,\n",
    "            'subsample_freq': 1,\n",
    "            'learning_rate': 0.04,\n",
    "            'feature_fraction': 0.9,\n",
    "            'max_depth': 15,\n",
    "            'lambda_l1': 1,  \n",
    "            'lambda_l2': 1,\n",
    "            'verbose': 50,\n",
    "            'early_stopping_rounds': 100, \n",
    "#             'eval_metric': lgb_kappa_eval,\n",
    "#             'eval_metric': ['qwk','qwk2','qwk3']\n",
    "            }\n",
    "\n",
    "cat_params = {'iterations':2000,\n",
    "            'loss_function': 'RMSE',\n",
    "            'verbose': 50,\n",
    "            'early_stopping_rounds': 100, \n",
    "#             'task_type':\"GPU\",\n",
    "#             'devices':'0:1',\n",
    "            }\n",
    "\n",
    "para_tune = {'feature_fraction': 0.9985706623844444, 'lambda_l1': 1.3464702887971034, 'lambda_l2': 24.03486401669668, 'max_depth': 9, 'num_leaves': 20, 'subsample': 0.9982720526777418, 'subsample_freq': 2}\n",
    "## 0.970\n",
    "# para_tune = {'feature_fraction': 0.7208784814394442, 'lambda_l1': 1.0547998031340668, 'lambda_l2': 47.046761174073815, 'max_depth': 15, 'num_leaves': 39, 'subsample': 0.9658561677957942, 'subsample_freq': 1}\n",
    "\n",
    "# for k in para_tune:\n",
    "#     lgb_params[k] = para_tune[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = reduce_train \n",
    "test = reduce_test\n",
    "print(train.columns[train.isnull().sum()>0])\n",
    "train = train.fillna(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y = train['accuracy_group']\n",
    "n_fold = 5\n",
    "folds = GroupKFold(n_splits=n_fold)\n",
    "\n",
    "cat_cols = ['title','world','timestamp_weekday']\n",
    "cols_to_drop = ['installation_id', 'game_session', 'timestamp', 'accuracy_group']\n",
    "cat_params['cat_cols'] = cat_cols\n",
    "lgb_params['cat_cols'] = cat_cols\n",
    "\n",
    "\n",
    "mt = MainTransformer()\n",
    "ft = FeatureTransformer()\n",
    "transformers = {'ft': ft}\n",
    "regressor_model1 = RegressorModel(model_wrapper=LGBWrapper_regr())\n",
    "regressor_model1.fit(X=train, y=y, folds=folds, params=lgb_params, preprocesser=mt, transformers=transformers,\n",
    "                    eval_metric='qwk', cols_to_drop=cols_to_drop, permutation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "* RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=cols_to_drop)\n",
    "col = X.columns[X.isnull().sum()>0]\n",
    "X = X.drop(columns=col)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = pd.DataFrame(StandardScaler().fit_transform(X),columns=X.columns)\n",
    "\n",
    "X_default = train.loc[:,['installation_id','timestamp','timestamp_weekday','timestamp_daytime','game_session','world','title','type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = lgb.LGBMRegressor().set_params(**lgb_params_rfe)\n",
    "selector = RFE(estimator, n_features_to_select=200, step=0.02)\n",
    "selector = selector.fit(X, y)\n",
    "select_col = X.columns[selector.get_support()] \n",
    "select_col = list(select_col.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature selection\n",
    "regressor_model2 = RegressorModel(columns=select_col+['installation_id'],model_wrapper=LGBWrapper_regr())\n",
    "regressor_model2.fit(X=train, y=y, folds=folds, params=lgb_params, preprocesser=mt, transformers=transformers,\n",
    "                    eval_metric='qwk', cols_to_drop=cols_to_drop, permutation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## semi-supervised learning\n",
    "coef = [1.12232214,1.73925866,2.22506454]\n",
    "y_test_pred = regressor_model1.predict(test)\n",
    "y_test_pred = cut_pred(y_test_pred,coef)\n",
    "X_append = test\n",
    "y_append = pd.Series(y_test_pred.flatten())\n",
    "\n",
    "regressor_model3 = RegressorModel(columns=select_col+['installation_id'],model_wrapper=LGBWrapper_regr())\n",
    "regressor_model3.fit(X=train, y=y, X_append=X_append, y_append=y_append, folds=folds, params=lgb_params, preprocesser=mt, transformers=transformers,\n",
    "                    eval_metric='qwk', cols_to_drop=cols_to_drop, permutation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "The preprocessing is a class, which was initially written by Abhishek Thakur here: https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/76107 and later improved here https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved (the improvement is is speed).\n",
    "\n",
    "It can be used to find optimal coefficients for thresholds. In this kernel I'll show an example, but when you do it, don't forget a proper validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import scipy as sp\n",
    "class OptimizedRounder(object):\n",
    "    \"\"\"\n",
    "    An optimizer for rounding thresholds\n",
    "    to maximize Quadratic Weighted Kappa (QWK) score\n",
    "    # https://www.kaggle.com/naveenasaithambi/optimizedrounder-improved\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        \"\"\"\n",
    "        Get loss according to\n",
    "        using current coefficients\n",
    "        \n",
    "        :param coef: A list of coefficients that will be used for rounding\n",
    "        :param X: The raw predictions\n",
    "        :param y: The ground truth labels\n",
    "        \"\"\"\n",
    "        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "\n",
    "        return -qwk_np(y, X_p)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Optimize rounding thresholds\n",
    "        \n",
    "        :param X: The raw predictions\n",
    "        :param y: The ground truth labels\n",
    "        \"\"\"\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        \"\"\"\n",
    "        Make predictions with specified thresholds\n",
    "        \n",
    "        :param X: The raw predictions\n",
    "        :param coef: A list of coefficients that will be used for rounding\n",
    "        \"\"\"\n",
    "        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n",
    "\n",
    "\n",
    "    def coefficients(self):\n",
    "        \"\"\"\n",
    "        Return the optimized coefficients\n",
    "        \"\"\"\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr1 = regressor_model1.predict(train)\n",
    "\n",
    "optR = OptimizedRounder()\n",
    "optR.fit(pr1.reshape(-1,), y)\n",
    "coefficients = optR.coefficients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_preds = optR.predict(pr1.reshape(-1, ), coefficients)\n",
    "qwk_np(y, opt_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr1 = cut_pred(pr1,coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(sample_submission, pred, submit_file_name='submission.csv'):\n",
    "    sample_submission['accuracy_group'] = pred.astype(int)\n",
    "    sample_submission.to_csv(submit_file_name, index=False)\n",
    "    print(sample_submission['accuracy_group'].value_counts(normalize=True))\n",
    "    \n",
    "make_submission(sample_submission=sample_submission, pred=pr1, submit_file_name='submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
