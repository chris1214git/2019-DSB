{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "缺失值\n",
    "各特徵是否有問題\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import copy\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# from tqdm import tqdm_notebook\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.svm import NuSVR, SVR\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# pd.options.display.precision = 15\n",
    "# from collections import defaultdict\n",
    "# import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "# import catboost as cat\n",
    "# # import timeb\n",
    "# from collections import Counter\n",
    "# import datetime\n",
    "# from catboost import CatBoostRegressor\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit, RepeatedStratifiedKFold\n",
    "# from sklearn import metrics\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# from sklearn import linear_model\n",
    "# import gc\n",
    "# import seaborn as sns\n",
    "# import warnings\n",
    "# # warnings.filterwarnings(\"ignore\")\n",
    "# from bayes_opt import BayesianOptimization\n",
    "# import eli5\n",
    "# import shap\n",
    "# from IPython.display import HTML\n",
    "# import json\n",
    "# import altair as alt\n",
    "# from category_encoders.ordinal import OrdinalEncoder\n",
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# from typing import List\n",
    "\n",
    "# import os\n",
    "# import time\n",
    "# import datetime\n",
    "# import json\n",
    "# import gc\n",
    "# from numba import jit\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from tqdm import tqdm_notebook\n",
    "\n",
    "# import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "# from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "# from sklearn import metrics\n",
    "# from typing import Any\n",
    "# from itertools import product\n",
    "# pd.set_option('max_rows', 500)\n",
    "# import re\n",
    "# from tqdm import tqdm\n",
    "# from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "from utils import kaggle_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/robikscube/2019-data-science-bowl-an-introduction\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "my_pal = sns.color_palette(n_colors=10)\n",
    "\n",
    "## https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_option.html#pandas.get_option\n",
    "pd.set_option('display.max_columns',100)\n",
    "pd.set_option('display.max_rows',100)\n",
    "\n",
    "pd.set_option('display.max_rows',10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 刪掉時間太少的gs\n",
    "\n",
    "def del_gs_by_time(train, del_time_th=10):\n",
    "    \"\"\"\n",
    "    del_time_th: delete game_session less than 10s\n",
    "    \"\"\"\n",
    "    print('\\nDeleteing gs by time ... ...')\n",
    "    ## 看兩次game_session時間差異\n",
    "    iid_gp = train.groupby(['installation_id'])\n",
    "    del_gs = []\n",
    "    for iid, gp in iid_gp:\n",
    "        x = gp.groupby(['game_session'],sort=False)['timestamp','type'].first()\n",
    "        x2 = x.timestamp\n",
    "        x2 = (x2 - x2.shift(1)).astype('timedelta64[s]')\n",
    "        del_gs += list(x.index.values[(x2<del_time_th) & (x['type']!='Assessment')])\n",
    "\n",
    "    print(len(del_gs))\n",
    "\n",
    "    ## 直接看game_time\n",
    "    train2 = train[(train['type']=='Activity') | (train['type']=='Game')]\n",
    "    x = train2.groupby(['game_session'])['game_time'].last()\n",
    "    del_gs2 = x[x<del_time_th*1000].index.values\n",
    "    print(len(del_gs2))\n",
    "\n",
    "    del train2\n",
    "    del_gs = set(del_gs)\n",
    "    del_gs.update(set(del_gs2))\n",
    "\n",
    "    print(len(del_gs))\n",
    "    del_id = train.game_session.isin(del_gs)\n",
    "    train2 = train.loc[~del_id]\n",
    "    return train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train.csv file....\n",
      "Training.csv file have 11341042 rows and 11 columns\n",
      "Reading test.csv file....\n",
      "Test.csv file have 1156414 rows and 11 columns\n",
      "Reading train_labels.csv file....\n",
      "Train_labels.csv file have 17690 rows and 7 columns\n",
      "Reading specs.csv file....\n",
      "Specs.csv file have 386 rows and 3 columns\n",
      "Reading sample_submission.csv file....\n",
      "Sample_submission.csv file have 1000 rows and 2 columns\n",
      "\n",
      "Deleteing gs by time ... ...\n",
      "2107\n",
      "7423\n",
      "9451\n",
      "\n",
      "Deleteing gs by time ... ...\n",
      "171\n",
      "716\n",
      "881\n",
      "(11341042, 11)\n",
      "(11305461, 11)\n",
      "(1156414, 11)\n",
      "(1153386, 11)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68804a50defd4691b76471eefd5887ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16968.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd4c40f710a4c22bd191eee4c6132a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "add_time_diff_hour ...\n",
      "add_time_diff_hour ...\n",
      "add_session_cnt ... ...\n",
      "add_session_cnt ... ...\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "train, test, train_labels, specs, sample_submission = kaggle_util.read_data()\n",
    "train.timestamp = pd.to_datetime(train.timestamp)\n",
    "test.timestamp = pd.to_datetime(test.timestamp)\n",
    "train1 = del_gs_by_time(train, del_time_th=10)\n",
    "test1 = del_gs_by_time(test, del_time_th=10)\n",
    "print(train.shape)\n",
    "print(train1.shape)\n",
    "print(test.shape)\n",
    "print(test1.shape)\n",
    "\n",
    "train2, test2, train_labels2 = train1.copy(), test1.copy(), train_labels.copy()\n",
    "\n",
    "# get usefull dict with maping encode, we implement \"label encoding\" on category features\n",
    "train2, test2, train_labels2, win_code, list_of_user_activities, list_of_event_code,\\\n",
    "activities_labels, assess_titles, list_of_event_id, all_title_event_code, activities_map = kaggle_util.encode_title(train2, test2, train_labels2)\n",
    "\n",
    "# tranform function to get the train and test set\n",
    "reduce_train, reduce_psuedo_train, reduce_test, categoricals = kaggle_util.get_train_and_test(train2, test2, win_code, list_of_user_activities, list_of_event_code,\\\n",
    "activities_labels, assess_titles, list_of_event_id, all_title_event_code, activities_map, create_psuedo_assessment=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce_train.to_csv('../data/preprocess/'+'train895.csv', index=False)\n",
    "# reduce_test.to_csv('../data/preprocess/'+'test895.csv', index=False)\n",
    "reduce_train.to_csv('../data/preprocess/'+'train_0106.csv', index=False)\n",
    "reduce_psuedo_train.to_csv('../data/preprocess/'+'psuedo_train_0106.csv', index=False)\n",
    "reduce_test.to_csv('../data/preprocess/'+'test_0106.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_train.session_cnt_30min.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 檢查有沒有缺失值\n",
    "reduce_train.columns[(reduce_train.isnull().sum()!=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 以使用者觀察\n",
    "iid_list = train.installation_id.unique()\n",
    "train[train['installation_id']==iid_list[10]].groupby(['game_session'])['title','type','timestamp'].first().sort_values('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 找重度使用者\n",
    "iid_gs_cnt = train.groupby(['installation_id'])['game_session'].nunique()\n",
    "tmp = iid_gs_cnt[iid_gs_cnt>300]\n",
    "print(tmp)\n",
    "\n",
    "##計算重度使用者比例\n",
    "print(train.game_session.nunique())\n",
    "print(np.sum(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.game_session.iloc[0]+'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 觀察重度使用者\n",
    "x = train[train['installation_id']=='08987c08'].groupby(['game_session'])['title','type','timestamp'].first().sort_values('timestamp')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train[train['installation_id']=='063e3e02'].groupby(['game_session'])['title','type','timestamp'].first().sort_values('timestamp')\n",
    "display(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=0\n",
    "for id, gp in reduce_train.groupby(['installation_id']):\n",
    "    cnt+=1\n",
    "    if (cnt)>500:\n",
    "        break\n",
    "    print(id)\n",
    "#     display(gp.iloc[:,-5:])\n",
    "    gp['title'] = gp['title'].map(activities_labels)\n",
    "    display((gp.loc[:,['title','accuracy_group']]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 檢查多人使用的情形\n",
    "## 查看多人使用的測資準確率多少?\n",
    "\n",
    "def describe_game_session(x,th=200):\n",
    "    total_gs = x.game_session.nunique()\n",
    "    id_gs = x.groupby(['installation_id'])['game_session'].nunique()\n",
    "    id_gs = id_gs[id_gs>th]\n",
    "    print(total_gs)\n",
    "    print(np.sum(id_gs))\n",
    "    print(id_gs)\n",
    "    \n",
    "describe_game_session(train,200)\n",
    "describe_game_session(test,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 依據type觀察event\n",
    "display(train[train['type']=='Clip'].iloc[:500])\n",
    "display(train[train['type']=='Activity'].iloc[:500])\n",
    "display(train[train['type']=='Game'].iloc[:500])\n",
    "display(train[train['type']=='Assessment'].iloc[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 觀察有哪些category資料在train上出現 test沒有(就是指太少出現)，應改掉\n",
    "## event id\n",
    "\n",
    "print(train.event_id.nunique())\n",
    "print(test.event_id.nunique())\n",
    "\n",
    "print(train.event_code.nunique())\n",
    "print(test.event_code.nunique())\n",
    "\n",
    "x = test.groupby(['event_id'])['game_session'].count()\n",
    "x = test.groupby(['event_code'])['game_session'].count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 檢查那些沒有label的使用者，有沒有足夠的使用紀錄\n",
    "iid_list = train.installation_id.unique()\n",
    "iid_list2 = train_labels.installation_id.unique()\n",
    "cnt=0\n",
    "for iid in iid_list:\n",
    "    if iid not in iid_list2:\n",
    "        cnt+=1\n",
    "        print(train[train['installation_id']==iid].game_session.nunique())\n",
    "        if cnt>10:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 依使用者觀察，以game_session為單位\n",
    "\n",
    "# id_all = train.groupby(['installation_id'])\n",
    "# cnt=0\n",
    "# for id1, x in id_all:\n",
    "#     cnt+=1\n",
    "#     if cnt>10:\n",
    "#         break\n",
    "#     print(id1)\n",
    "#     x2 = x.groupby(['game_session']).first()\n",
    "#     print(x2.loc[:,['title','type','world','timestamp']])\n",
    "\n",
    "# # reduce_train.loc[:,['installation_id','timestamp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a={1:0,2:2}\n",
    "np.sum(a.values())\n",
    "sum(a.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a={1:0,2:2}\n",
    "features = dict()\n",
    "features[1] = a[1]\n",
    "features\n",
    "a[1]=4\n",
    "features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(filter(lambda x: x > 0, a.values()))\n",
    "np.sum([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(assess_to_title).to_csv('../data/preprocess/assess_to_title.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv('../data/raw_data/media_sequence.csv')\n",
    "assess_to_title = dict()\n",
    "tmp = []\n",
    "for i in range(1,x.shape[0]):\n",
    "    tmp.append(x.loc[i,'title'])\n",
    "    if x.loc[i,'type'] == 'Assessment':\n",
    "        assess_to_title['{}'.format(x.loc[i,'title'])]=tmp\n",
    "        tmp = []\n",
    "assess_to_title"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
